{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries, define constants, functions, and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../3_train_and_test_models\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from params import ROOT, SPECIES, TFS\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* paths and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{ROOT}/plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_HOLDOUTS    = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preds_file(test_species, tf, num_holdout):\n",
    "    preds_root = ROOT + \"/model_out\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/EvoGS-{num_holdout}_{tf}_{test_species}-tested.preds.npy\"\n",
    "\n",
    "def get_labels_file(test_species, tf, num_holdout):\n",
    "    preds_root = ROOT + \"/model_out\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/EvoGS-{num_holdout}_{tf}_{test_species}-tested.labels.npy\"\n",
    "\n",
    "def load_all_test_sets():\n",
    "\n",
    "    preds_dict  = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "    labels_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "    for tf in TFS:\n",
    "        for num_holdout in NUM_HOLDOUTS:\n",
    "            print(f\"=== {tf}-Evo with {num_holdout} holdouts ===\")\n",
    "            preds_file  = get_preds_file(tf=tf, test_species=\"hg38\", num_holdout=num_holdout)\n",
    "            labels_file = get_labels_file(tf=tf, test_species=\"hg38\", num_holdout=num_holdout)\n",
    "            preds_dict[tf][\"EvoGS\"][num_holdout]    = np.load(preds_file)\n",
    "            labels_dict[tf][\"EvoGS\"][num_holdout]   = np.load(labels_file)\n",
    "\n",
    "    return preds_dict, labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_seaborn(auPRC_dicts, auROC_dicts):\n",
    "\n",
    "    # This function re-formats the performance dictionaries into\n",
    "    # one pandas DataFrame that matches how seaborn expects.\n",
    "    tf_col      = []\n",
    "    holdout_col = []\n",
    "    auprc_col   = []\n",
    "    auroc_col   = []\n",
    "    for tf in TFS:\n",
    "        for num_holdout in NUM_HOLDOUTS:\n",
    "            tf_col.extend([tf])\n",
    "            holdout_col.extend([num_holdout])\n",
    "\n",
    "            # Now grab the five-fold for the current TF and domain\n",
    "            auprc_col.extend([auPRC_dicts[tf][\"EvoGS\"][num_holdout]])\n",
    "            auroc_col.extend([auROC_dicts[tf][\"EvoGS\"][num_holdout]])\n",
    "        \n",
    "    return pd.DataFrame({\"TF\":tf_col, \"Holdouts\":holdout_col, \"auPRC\":auprc_col, \"auROC\":auroc_col})\n",
    "\n",
    "def get_auPRCs(labels, preds):\n",
    "    # This function calculates the auPRC for each set of\n",
    "    # predictions passed in. The length of the 2nd axis\n",
    "    # of the predictions array passed in will be the # of\n",
    "    # auPRCs returned as a list. The length of the 1st axis\n",
    "    # of the predictions array should match the length\n",
    "    # of the labels array.\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Truncating so that {len(preds)} matches {len(labels)}\")\n",
    "\n",
    "    # (1) Adjust the label length to match the prediction length\n",
    "    len_to_truncate_by = preds.shape[0]\n",
    "\n",
    "    labels = labels[:len_to_truncate_by]\n",
    "\n",
    "    # (3) Calculate the AUC-ROC and AUC-PR for each of the five-folds\n",
    "    return average_precision_score(labels, preds)\n",
    "\n",
    "def get_auROCs(labels, preds):\n",
    "    # This function calculates the auROC for each set of\n",
    "    # predictions passed in. The length of the 2nd axis\n",
    "    # of the predictions array passed in will be the # of\n",
    "    # auROCs returned as a list. The length of the 1st axis\n",
    "    # of the predictions array should match the length\n",
    "    # of the labels array.\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Truncating so that {len(preds)} matches {len(labels)}\")\n",
    "\n",
    "    # (1) Adjust the label length to match the prediction length\n",
    "    len_to_truncate_by = preds.shape[0]\n",
    "\n",
    "    labels = labels[:len_to_truncate_by]\n",
    "\n",
    "    # (3) Calculate the AUC-ROC and AUC-PR for each of the five-folds\n",
    "    return roc_auc_score(labels, preds)\n",
    "\n",
    "def get_performance_df(preds_dict, labels_dict):\n",
    "\n",
    "    # This function loads in binding labels for each TF for \n",
    "    # a given test species, and for each TF, calculates the auPRC\n",
    "    # using each set of predictions that is input in \"preds_dict\".\n",
    "    auPRC_dicts = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "    auROC_dicts = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "    for tf in TFS:\n",
    "        for num_holdout in NUM_HOLDOUTS:\n",
    "            model_preds = preds_dict[tf][\"EvoGS\"][num_holdout].squeeze()\n",
    "            labels      = labels_dict[tf][\"EvoGS\"][num_holdout].squeeze()\n",
    "            \n",
    "            # Save predictions\n",
    "            auPRC_dicts[tf][\"EvoGS\"][num_holdout] = get_auPRCs(labels, model_preds)\n",
    "            auROC_dicts[tf][\"EvoGS\"][num_holdout] = get_auROCs(labels, model_preds)\n",
    "\n",
    "    # Before returning all the auPRCs in dictionaries,\n",
    "    # we just need to reformat how they are stored\n",
    "    # because seaborn expects particularly formatted input\n",
    "    return format_data_for_seaborn(auPRC_dicts, auROC_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save into usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CEBPA-Evo with 0 holdouts ===\n",
      "=== CEBPA-Evo with 1 holdouts ===\n",
      "=== CEBPA-Evo with 2 holdouts ===\n",
      "=== CEBPA-Evo with 3 holdouts ===\n",
      "=== FOXA1-Evo with 0 holdouts ===\n",
      "=== FOXA1-Evo with 1 holdouts ===\n",
      "=== FOXA1-Evo with 2 holdouts ===\n",
      "=== FOXA1-Evo with 3 holdouts ===\n",
      "=== HNF4A-Evo with 0 holdouts ===\n",
      "=== HNF4A-Evo with 1 holdouts ===\n",
      "=== HNF4A-Evo with 2 holdouts ===\n",
      "=== HNF4A-Evo with 3 holdouts ===\n",
      "=== HNF6-Evo with 0 holdouts ===\n",
      "=== HNF6-Evo with 1 holdouts ===\n",
      "=== HNF6-Evo with 2 holdouts ===\n",
      "=== HNF6-Evo with 3 holdouts ===\n"
     ]
    }
   ],
   "source": [
    "preds, labels   = load_all_test_sets()\n",
    "performance_df  = get_performance_df(preds_dict=preds, labels_dict=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TF  Holdouts     auPRC     auROC\n",
      "0   CEBPA         0  0.328087  0.948873\n",
      "1   CEBPA         1  0.290576  0.949429\n",
      "2   CEBPA         2  0.256166  0.937954\n",
      "3   CEBPA         3  0.185736  0.914419\n",
      "4   FOXA1         0  0.275479  0.938285\n",
      "5   FOXA1         1  0.233901  0.924876\n",
      "6   FOXA1         2  0.210019  0.917840\n",
      "7   FOXA1         3  0.124702  0.874638\n",
      "8   HNF4A         0  0.338122  0.940231\n",
      "9   HNF4A         1  0.310496  0.934079\n",
      "10  HNF4A         2  0.281974  0.924734\n",
      "11  HNF4A         3  0.244872  0.914602\n",
      "12   HNF6         0  0.192555  0.950243\n",
      "13   HNF6         1  0.175740  0.944592\n",
      "14   HNF6         2  0.137246  0.927308\n",
      "15   HNF6         3  0.093188  0.899697\n"
     ]
    }
   ],
   "source": [
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv(ROOT + f\"/plots/Figure6/evo-group-species_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomic_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
