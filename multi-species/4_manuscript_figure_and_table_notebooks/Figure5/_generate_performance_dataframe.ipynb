{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  3 21:53:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L40                     Off |   00000000:A5:00.0 Off |                    0 |\n",
      "| N/A   28C    P8             34W /  300W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries, define constants, functions, and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../3_train_and_test_models\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from params import ROOT, SPECIES, TFS\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* paths and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{ROOT}/plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shorthand names for all model types to include in plots\n",
    "MODELS = [\n",
    "    \"Baseline\",\n",
    "    \"BM\",\n",
    "    \"MORALE\"\n",
    "]\n",
    "\n",
    "TFS = [\"CEBPA\", \"FOXA1\", \"HNF6\", \"HNF4A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_file(model, tf, test_species):\n",
    "\n",
    "    assert model in MODELS, f\"Model {model} not found. Please choose from {MODELS}\"\n",
    "\n",
    "    model_path = ROOT + \"/\".join([\"/models\", tf, test_species + \"_tested\", f\"{model}/\"])\n",
    "\n",
    "    match model:\n",
    "        case \"Baseline\":\n",
    "            model_file_suffix = \".baseline.pt\"\n",
    "\n",
    "            # get all files that match the prefix and suffix\n",
    "            files = [f for f in os.listdir(model_path) if f.endswith(model_file_suffix)]\n",
    "            \n",
    "            # sort files and return the one that is most recent\n",
    "            latest_file = max([model_path + f for f in files], key=os.path.getctime)\n",
    "\n",
    "            return latest_file\n",
    "\n",
    "        case \"BM\":\n",
    "            model_file_suffix = \".basic_model.pt\"\n",
    "\n",
    "            # get all files that match the prefix and suffix\n",
    "            files = [f for f in os.listdir(model_path) if f.endswith(model_file_suffix)]\n",
    "            \n",
    "            # sort files and return the one that is most recent\n",
    "            latest_file = max([model_path + f for f in files], key=os.path.getctime)\n",
    "\n",
    "            return latest_file\n",
    "\n",
    "        case \"MORALE\":\n",
    "            feature_extractor_suffix=\".feature_extractor.pt\"\n",
    "            classifier_suffix= \".classifier.pt\"\n",
    "\n",
    "            # get all files that match the prefix and suffix\n",
    "            classifier_files        = [f for f in os.listdir(model_path) if f.endswith(classifier_suffix)]\n",
    "            feature_extractor_files = [f for f in os.listdir(model_path) if f.endswith(feature_extractor_suffix)]\n",
    "            \n",
    "            # sort files and return the one that is most recent\n",
    "            latest_classifier_file          = max([model_path + f for f in classifier_files], key=os.path.getctime)\n",
    "            latest_feature_extractor_file   = max([model_path + f for f in feature_extractor_files], key=os.path.getctime)\n",
    "\n",
    "            return latest_feature_extractor_file, latest_classifier_file\n",
    "\n",
    "        case _:\n",
    "            print(\"Not implemented yet\")\n",
    "            exit(1)\n",
    "\n",
    "def get_preds_file(model, tf, test_species):\n",
    "\n",
    "    assert model in MODELS, f\"Model {model} not found. Please choose from {MODELS}\"\n",
    "\n",
    "    preds_root = ROOT + \"/model_out\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "\n",
    "    match model:\n",
    "        case \"Baseline\":\n",
    "            pred_file = f\"{preds_root}/Baseline_{tf}_{test_species}-tested.preds.npy\"\n",
    "\n",
    "        case \"BM\":\n",
    "            pred_file = f\"{preds_root}/BM_{tf}_{test_species}-tested.preds.npy\"\n",
    "\n",
    "        case \"MORALE\":\n",
    "            pred_file = f\"{preds_root}/MORALE_{tf}_{test_species}-tested.preds.npy\"\n",
    "            \n",
    "        case _:\n",
    "            print(\"Not implemented yet\")\n",
    "            exit(1)\n",
    "\n",
    "    return pred_file\n",
    "\n",
    "def get_labels_file(model, tf, test_species):\n",
    "\n",
    "    assert model in MODELS, f\"Model {model} not found. Please choose from {MODELS}\"\n",
    "    \n",
    "    preds_root = ROOT + \"/model_out\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "\n",
    "    match model:\n",
    "        case \"Baseline\":\n",
    "            labels_file = f\"{preds_root}/Baseline_{tf}_{test_species}-tested.labels.npy\"\n",
    "\n",
    "        case \"BM\":\n",
    "            labels_file = f\"{preds_root}/BM_{tf}_{test_species}-tested.labels.npy\"\n",
    "\n",
    "        case \"MORALE\":\n",
    "            labels_file = f\"{preds_root}/MORALE_{tf}_{test_species}-tested.labels.npy\"\n",
    "\n",
    "        case _:\n",
    "            print(\"Not implemented yet\")\n",
    "            exit(1)\n",
    "\n",
    "    return labels_file\n",
    "\n",
    "def load_all_test_sets():\n",
    "    preds_dict  = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "    labels_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "\n",
    "    # Loop over mouse-trained, human-trained models, and domain-adaptive models\n",
    "    for model in MODELS:\n",
    "        for species in SPECIES:\n",
    "            for tf in TFS:\n",
    "                print(f\"=== {tf}-{model} tested in {species} ===\")\n",
    "                preds_file  = get_preds_file(model=model, tf=tf, test_species=species)\n",
    "                labels_file = get_labels_file(model=model, tf=tf, test_species=species)\n",
    "                preds_dict[model][tf][species]   = np.load(preds_file)\n",
    "                labels_dict[model][tf][species]  = np.load(labels_file)\n",
    "\n",
    "    return preds_dict, labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_seaborn(auPRC_dicts, auROC_dicts):\n",
    "    # This function re-formats the performance dictionaries into\n",
    "    # one pandas DataFrame that matches how seaborn expects.\n",
    "    tf_col      = []\n",
    "    model_col   = []\n",
    "    species_col = []\n",
    "    auprc_col   = []\n",
    "    auroc_col   = []\n",
    "\n",
    "    model_list  = list(auPRC_dicts.keys())\n",
    "\n",
    "    for model in MODELS:\n",
    "        for tf in TFS:\n",
    "            for species in SPECIES:\n",
    "                # We extend these based on the entires from the five-fold cross validation\n",
    "                model_col.extend([model])\n",
    "                tf_col.extend([tf])\n",
    "                species_col.extend([species])\n",
    "\n",
    "                # Now grab the five-fold for the current TF and domain\n",
    "                auprc_col.extend([auPRC_dicts[model][tf][species]])\n",
    "                auroc_col.extend([auROC_dicts[model][tf][species]])\n",
    "        \n",
    "    return pd.DataFrame({\"TF\":tf_col, \"Model\":model_col, \"Target\": species_col, \"auPRC\":auprc_col, \"auROC\":auroc_col})\n",
    "\n",
    "def get_auPRCs(labels, preds):\n",
    "    # This function calculates the auPRC for each set of\n",
    "    # predictions passed in. The length of the 2nd axis\n",
    "    # of the predictions array passed in will be the # of\n",
    "    # auPRCs returned as a list. The length of the 1st axis\n",
    "    # of the predictions array should match the length\n",
    "    # of the labels array.        \n",
    "\n",
    "    # (1) Adjust the label length to match the prediction length\n",
    "    len_to_truncate_by = preds.shape[0]\n",
    "\n",
    "    labels = labels[:len_to_truncate_by]\n",
    "\n",
    "    # (3) Calculate the AUC-ROC and AUC-PR for each of the five-folds\n",
    "    return average_precision_score(labels, preds)\n",
    "\n",
    "def get_auROCs(labels, preds):\n",
    "    # This function calculates the auROC for each set of\n",
    "    # predictions passed in. The length of the 2nd axis\n",
    "    # of the predictions array passed in will be the # of\n",
    "    # auROCs returned as a list. The length of the 1st axis\n",
    "    # of the predictions array should match the length\n",
    "    # of the labels array.\n",
    "\n",
    "    # (1) Adjust the label length to match the prediction length\n",
    "    len_to_truncate_by = preds.shape[0]\n",
    "\n",
    "    labels = labels[:len_to_truncate_by]\n",
    "\n",
    "    # (3) Calculate the AUC-ROC and AUC-PR for each of the five-folds\n",
    "    return roc_auc_score(labels, preds)\n",
    "\n",
    "def get_performance_df(preds_dict, labels_dict):\n",
    "    # This function loads in binding labels for each TF for \n",
    "    # a given test species, and for each TF, calculates the auPRC\n",
    "    # using each set of predictions that is input in \"preds_dict\".\n",
    "    auPRC_dicts = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "    auROC_dicts = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "\n",
    "    for model in MODELS:\n",
    "        for tf in TFS:\n",
    "            for species in SPECIES:\n",
    "                model_preds = preds_dict[model][tf][species].squeeze()\n",
    "                labels      = labels_dict[model][tf][species]\n",
    "                \n",
    "                # Save predictions\n",
    "                auPRC_dicts[model][tf][species] = get_auPRCs(labels, model_preds)\n",
    "                auROC_dicts[model][tf][species] = get_auROCs(labels, model_preds)\n",
    "\n",
    "    # Before returning all the auPRCs in dictionaries,\n",
    "    # we just need to reformat how they are stored\n",
    "    # because seaborn expects particularly formatted input\n",
    "    return format_data_for_seaborn(auPRC_dicts, auROC_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save into usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CEBPA-Baseline tested in mm10 ===\n",
      "=== FOXA1-Baseline tested in mm10 ===\n",
      "=== HNF6-Baseline tested in mm10 ===\n",
      "=== HNF4A-Baseline tested in mm10 ===\n",
      "=== CEBPA-Baseline tested in hg38 ===\n",
      "=== FOXA1-Baseline tested in hg38 ===\n",
      "=== HNF6-Baseline tested in hg38 ===\n",
      "=== HNF4A-Baseline tested in hg38 ===\n",
      "=== CEBPA-Baseline tested in rheMac10 ===\n",
      "=== FOXA1-Baseline tested in rheMac10 ===\n",
      "=== HNF6-Baseline tested in rheMac10 ===\n",
      "=== HNF4A-Baseline tested in rheMac10 ===\n",
      "=== CEBPA-Baseline tested in canFam6 ===\n",
      "=== FOXA1-Baseline tested in canFam6 ===\n",
      "=== HNF6-Baseline tested in canFam6 ===\n",
      "=== HNF4A-Baseline tested in canFam6 ===\n",
      "=== CEBPA-Baseline tested in rn7 ===\n",
      "=== FOXA1-Baseline tested in rn7 ===\n",
      "=== HNF6-Baseline tested in rn7 ===\n",
      "=== HNF4A-Baseline tested in rn7 ===\n",
      "=== CEBPA-BM tested in mm10 ===\n",
      "=== FOXA1-BM tested in mm10 ===\n",
      "=== HNF6-BM tested in mm10 ===\n",
      "=== HNF4A-BM tested in mm10 ===\n",
      "=== CEBPA-BM tested in hg38 ===\n",
      "=== FOXA1-BM tested in hg38 ===\n",
      "=== HNF6-BM tested in hg38 ===\n",
      "=== HNF4A-BM tested in hg38 ===\n",
      "=== CEBPA-BM tested in rheMac10 ===\n",
      "=== FOXA1-BM tested in rheMac10 ===\n",
      "=== HNF6-BM tested in rheMac10 ===\n",
      "=== HNF4A-BM tested in rheMac10 ===\n",
      "=== CEBPA-BM tested in canFam6 ===\n",
      "=== FOXA1-BM tested in canFam6 ===\n",
      "=== HNF6-BM tested in canFam6 ===\n",
      "=== HNF4A-BM tested in canFam6 ===\n",
      "=== CEBPA-BM tested in rn7 ===\n",
      "=== FOXA1-BM tested in rn7 ===\n",
      "=== HNF6-BM tested in rn7 ===\n",
      "=== HNF4A-BM tested in rn7 ===\n",
      "=== CEBPA-MORALE tested in mm10 ===\n",
      "=== FOXA1-MORALE tested in mm10 ===\n",
      "=== HNF6-MORALE tested in mm10 ===\n",
      "=== HNF4A-MORALE tested in mm10 ===\n",
      "=== CEBPA-MORALE tested in hg38 ===\n",
      "=== FOXA1-MORALE tested in hg38 ===\n",
      "=== HNF6-MORALE tested in hg38 ===\n",
      "=== HNF4A-MORALE tested in hg38 ===\n",
      "=== CEBPA-MORALE tested in rheMac10 ===\n",
      "=== FOXA1-MORALE tested in rheMac10 ===\n",
      "=== HNF6-MORALE tested in rheMac10 ===\n",
      "=== HNF4A-MORALE tested in rheMac10 ===\n",
      "=== CEBPA-MORALE tested in canFam6 ===\n",
      "=== FOXA1-MORALE tested in canFam6 ===\n",
      "=== HNF6-MORALE tested in canFam6 ===\n",
      "=== HNF4A-MORALE tested in canFam6 ===\n",
      "=== CEBPA-MORALE tested in rn7 ===\n",
      "=== FOXA1-MORALE tested in rn7 ===\n",
      "=== HNF6-MORALE tested in rn7 ===\n",
      "=== HNF4A-MORALE tested in rn7 ===\n"
     ]
    }
   ],
   "source": [
    "preds, labels   = load_all_test_sets()\n",
    "performance_df  = get_performance_df(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TF     Model    Target     auPRC     auROC\n",
      "0   CEBPA  Baseline      mm10  0.293234  0.926231\n",
      "1   CEBPA  Baseline      hg38  0.229496  0.932284\n",
      "2   CEBPA  Baseline  rheMac10  0.140205  0.941620\n",
      "3   CEBPA  Baseline   canFam6  0.316352  0.929087\n",
      "4   CEBPA  Baseline       rn7  0.293075  0.932355\n",
      "5   FOXA1  Baseline      mm10  0.289953  0.914846\n",
      "6   FOXA1  Baseline      hg38  0.183770  0.912921\n",
      "7   FOXA1  Baseline  rheMac10  0.184296  0.921451\n",
      "8   FOXA1  Baseline   canFam6  0.164992  0.909328\n",
      "9   FOXA1  Baseline       rn7  0.266658  0.908988\n",
      "10   HNF6  Baseline      mm10  0.311277  0.925725\n",
      "11   HNF6  Baseline      hg38  0.131341  0.911898\n",
      "12   HNF6  Baseline  rheMac10  0.124664  0.937557\n",
      "13   HNF6  Baseline   canFam6  0.069513  0.889983\n",
      "14   HNF6  Baseline       rn7  0.161331  0.926982\n",
      "15  HNF4A  Baseline      mm10  0.432209  0.917187\n",
      "16  HNF4A  Baseline      hg38  0.261007  0.923981\n",
      "17  HNF4A  Baseline  rheMac10  0.208617  0.942256\n",
      "18  HNF4A  Baseline   canFam6  0.266069  0.905339\n",
      "19  HNF4A  Baseline       rn7  0.301606  0.935563\n",
      "20  CEBPA        BM      mm10  0.329759  0.937688\n",
      "21  CEBPA        BM      hg38  0.312705  0.946750\n",
      "22  CEBPA        BM  rheMac10  0.228558  0.972400\n",
      "23  CEBPA        BM   canFam6  0.330214  0.931464\n",
      "24  CEBPA        BM       rn7  0.331039  0.938470\n",
      "25  FOXA1        BM      mm10  0.317963  0.919444\n",
      "26  FOXA1        BM      hg38  0.264889  0.936842\n",
      "27  FOXA1        BM  rheMac10  0.268980  0.946465\n",
      "28  FOXA1        BM   canFam6  0.241157  0.937452\n",
      "29  FOXA1        BM       rn7  0.301904  0.915822\n",
      "30   HNF6        BM      mm10  0.275140  0.914944\n",
      "31   HNF6        BM      hg38  0.190812  0.949999\n",
      "32   HNF6        BM  rheMac10  0.241383  0.969820\n",
      "33   HNF6        BM   canFam6  0.153546  0.919902\n",
      "34   HNF6        BM       rn7  0.225908  0.942710\n",
      "35  HNF4A        BM      mm10  0.412401  0.916611\n",
      "36  HNF4A        BM      hg38  0.333308  0.935055\n",
      "37  HNF4A        BM  rheMac10  0.326970  0.962322\n",
      "38  HNF4A        BM   canFam6  0.287582  0.908355\n",
      "39  HNF4A        BM       rn7  0.358354  0.948029\n",
      "40  CEBPA    MORALE      mm10  0.333161  0.935112\n",
      "41  CEBPA    MORALE      hg38  0.323714  0.944821\n",
      "42  CEBPA    MORALE  rheMac10  0.237806  0.971809\n",
      "43  CEBPA    MORALE   canFam6  0.330391  0.930879\n",
      "44  CEBPA    MORALE       rn7  0.338940  0.941377\n",
      "45  FOXA1    MORALE      mm10  0.331980  0.926083\n",
      "46  FOXA1    MORALE      hg38  0.272544  0.939887\n",
      "47  FOXA1    MORALE  rheMac10  0.277983  0.949394\n",
      "48  FOXA1    MORALE   canFam6  0.242969  0.937848\n",
      "49  FOXA1    MORALE       rn7  0.307562  0.919349\n",
      "50   HNF6    MORALE      mm10  0.273576  0.917483\n",
      "51   HNF6    MORALE      hg38  0.201287  0.950069\n",
      "52   HNF6    MORALE  rheMac10  0.200334  0.969360\n",
      "53   HNF6    MORALE   canFam6  0.137005  0.922424\n",
      "54   HNF6    MORALE       rn7  0.221348  0.944803\n",
      "55  HNF4A    MORALE      mm10  0.424625  0.921571\n",
      "56  HNF4A    MORALE      hg38  0.337437  0.936759\n",
      "57  HNF4A    MORALE  rheMac10  0.325306  0.960534\n",
      "58  HNF4A    MORALE   canFam6  0.287221  0.911223\n",
      "59  HNF4A    MORALE       rn7  0.352702  0.946811\n"
     ]
    }
   ],
   "source": [
    "print(performance_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv(ROOT + f\"/plots/performance_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomic_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
