{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/talisker/home/benos/mae117/.conda/envs/crested/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(f\"../../3_train_and_test_models\")\n",
    "\n",
    "import keras\n",
    "from keras.layers import TorchModuleWrapper\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import random\n",
    "import crested\n",
    "\n",
    "from math import floor, ceil\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "from typing import Callable, List, Optional, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "# Different libraries to get attributions\n",
    "#from captum.attr import IntegratedGradients\n",
    "#from tangermeme.deep_lift_shap import deep_lift_shap\n",
    "from crested.tl import contribution_scores\n",
    "\n",
    "# Grab things we need from other models\n",
    "from params import Params, ROOT, TFS, SPECIES, GENOMES\n",
    "from Baseline.test import ConvHead, LinearBlock, FeedForwardBlock, GRUBlock\n",
    "from Baseline.test import BasicModel as BaselineModel\n",
    "from MORALE.test import FeatureExtractor, Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to do this because all of our models contain a GRU element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.backends.cudnn.enabled = False # LSTM, GRU, and RNNs are not supported by cuDNN for captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_file(model, params):\n",
    "\n",
    "    model_path = ROOT + \"/\".join([\"/models\", params.tf, params.target_species + \"_tested\", f\"{model}/\"])\n",
    "\n",
    "    # Need to get all files that match the specief prefix (model type). If we use MORALE, \n",
    "    # we have to return the feature extractor and the classifier models.\n",
    "    if model == \"Baseline\":\n",
    "        model_file_suffix = \".baseline.pt\"\n",
    "        files = [f for f in os.listdir(model_path) if f.endswith(model_file_suffix)]\n",
    "        latest_file = max([model_path + f for f in files], key=os.path.getctime)\n",
    "        return latest_file\n",
    "    elif model == \"BM\":\n",
    "        model_file_suffix = \".basic_model.pt\"\n",
    "        files = [f for f in os.listdir(model_path) if f.endswith(model_file_suffix)]\n",
    "        latest_file = max([model_path + f for f in files], key=os.path.getctime)\n",
    "        return latest_file\n",
    "    elif \"EvoPS\" in model or model == \"MORALE\":\n",
    "        fe_file_suffix = \".feature_extractor.pt\"\n",
    "        cl_file_suffix = \".classifier.pt\"\n",
    "        \n",
    "        fe_files = [f for f in os.listdir(model_path) if f.endswith(fe_file_suffix)]\n",
    "        cl_files = [f for f in os.listdir(model_path) if f.endswith(cl_file_suffix)]\n",
    "\n",
    "        latest_fe_file = max([model_path + f for f in fe_files], key=os.path.getctime)\n",
    "        latest_cl_file = max([model_path + f for f in cl_files], key=os.path.getctime)\n",
    "        \n",
    "        return latest_fe_file, latest_cl_file\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_file(tf, target_species):\n",
    "    preds_root = ROOT + \"/model_out\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/BM_{tf}_{target_species}-tested.preds.npy\"\n",
    "\n",
    "def get_labels_file(tf, target_species):    \n",
    "    preds_root = ROOT + \"/model_out\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/Baseline_{tf}_{target_species}-tested.labels.npy\"\n",
    "\n",
    "def load_performance_data(verbose=False):\n",
    "    preds_dict      = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    labels_dict     = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    bound_indices   = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    unbound_indices = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "\n",
    "    # Do for each tf-species pair\n",
    "    for tf in TFS:\n",
    "        for target in SPECIES:\n",
    "\n",
    "            preds_file  = get_preds_file(tf=tf, target_species=target)\n",
    "            labels_file = get_labels_file(tf=tf, target_species=target)\n",
    "            try:\n",
    "                # Load them\n",
    "                preds = np.load(preds_file).squeeze()\n",
    "                labels = np.load(labels_file).squeeze()\n",
    "\n",
    "                # Calculate if we need to truncate the labels\n",
    "                if preds.shape[0] != labels.shape[0]:\n",
    "                    print(\"\\t\\t Preds & labels mismatch! truncating labels\\n\")\n",
    "                    labels = labels[:preds.shape[0]]\n",
    "\n",
    "                assert preds.shape[0] == labels.shape[0]\n",
    "\n",
    "                # We save predictions from each of the five-folds per model, TF, source, and target\n",
    "                preds_dict[tf][target]  = preds             \n",
    "                labels_dict[tf][target] = labels\n",
    "\n",
    "                # Store unbound and bound indices for all models, TFs, sources, and targets\n",
    "                bound_indices[tf][target]   = np.nonzero(labels == 1)[0]\n",
    "                unbound_indices[tf][target] = np.nonzero(labels == 0)[0]\n",
    "            except:\n",
    "                print(\"Could not load regular preds/labels files\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\t\\t\\t---> Generated dictionaries needed for cnf matrix construction!\\n\")\n",
    "    \n",
    "    return preds_dict, labels_dict, bound_indices, unbound_indices\n",
    "\n",
    "def generate_confusion_matrix(verbose=False, bound_threshold=0.98, unbound_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Since we are generating attribtuions from bound sites for each TF, we characterize this \n",
    "    confusion matrix as one that 'finds' the most extreme sites (i.e. for those that are bound,\n",
    "    sites that all folds agree as bound (>0.8 is our default)).\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\t\\t--> Generating TPs, FPs, TNS, and FNs by looking at bound/unbound sites (in comparison to the predictions).\\n\")\n",
    "\n",
    "    # (1) Load the predictions, labels, and bound/unbound indices\n",
    "    preds_dict, labels_dict, bound_indices, unbound_indices = load_performance_data(verbose=verbose)\n",
    "\n",
    "    # Indices we need to populate\n",
    "    ex_tp_indices   = defaultdict(lambda: {})\n",
    "    ex_fp_indices   = defaultdict(lambda: {})\n",
    "    ex_tn_indices   = defaultdict(lambda: {})\n",
    "    ex_fn_indices   = defaultdict(lambda: {})\n",
    "\n",
    "    # (2) Generate the confusion matrix\n",
    "    for tf in TFS:\n",
    "        for target in SPECIES:\n",
    "\n",
    "            # We use <= 0.5 to classify as unbound \n",
    "            bound_predictions_indices       = np.nonzero(preds_dict[tf][target] > bound_threshold)[0]\n",
    "            unbound_predictions_indices     = np.nonzero(preds_dict[tf][target] <= unbound_threshold)[0]\n",
    "\n",
    "            # We store these for each example, but they will be constant across target-tfs pairs\n",
    "            ground_truth_bound_indices      = bound_indices[tf][target]\n",
    "            ground_truth_unbound_indices    = unbound_indices[tf][target]\n",
    "\n",
    "            # For each example the models predicted as bound...\n",
    "            ex_tp_indices[tf][target] = set(bound_predictions_indices).intersection(set(ground_truth_bound_indices))\n",
    "            ex_fp_indices[tf][target] = set(bound_predictions_indices).intersection(set(ground_truth_unbound_indices))\n",
    "\n",
    "            # For each example the models predicted as bound...\n",
    "            ex_tn_indices[tf][target] = set(unbound_predictions_indices).intersection(set(ground_truth_unbound_indices))\n",
    "            ex_fn_indices[tf][target] = set(unbound_predictions_indices).intersection(set(ground_truth_bound_indices))\n",
    "\n",
    "    return ex_tp_indices, ex_fp_indices, ex_tn_indices, ex_fn_indices\n",
    "\n",
    "def get_agreement_sites(params, verbose=False):\n",
    "    \"\"\" \n",
    "    We capture and zero-in on the extreme sites that are strongly predicted in the target\n",
    "    species. We look at the Baseline model in order to do this.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\t-> Finding the sites to use for attribution\\n\")\n",
    "\n",
    "    assert params.target_species in SPECIES, f\"Target species must be one of {SPECIES}\"\n",
    "\n",
    "    ex_tp_indices, ex_fp_indices, ex_tn_indices, ex_fn_indices = generate_confusion_matrix(verbose=verbose)\n",
    "        \n",
    "    extreme_agreement_sites = defaultdict(lambda : dict())\n",
    "    for tf in TFS:\n",
    "        extreme_agreement_sites[tf][\"TP\"] = ex_tp_indices[tf][params.target_species]\n",
    "        extreme_agreement_sites[tf][\"FP\"] = ex_fp_indices[tf][params.target_species]\n",
    "        extreme_agreement_sites[tf][\"TN\"] = ex_tn_indices[tf][params.target_species]\n",
    "        extreme_agreement_sites[tf][\"FN\"] = ex_fn_indices[tf][params.target_species]\n",
    "\n",
    "    return extreme_agreement_sites\n",
    "\n",
    "def read_holdout_bed(holdout_type, params, verbose=True):\n",
    "\n",
    "    assert holdout_type in [\"test\", \"val\"], f\"Invalid holdout type {holdout_type}. Please choose from ['test', 'val']\"\n",
    "\n",
    "    print(f\"> Converting sites from the {holdout_type} set so that we can use them to get model attributions.\\n\")\n",
    "\n",
    "    holdout_bed_path    = f\"{ROOT}/data/{params.target_species}/{params.tf}/{holdout_type}_shuf.bed\"\n",
    "    holdout_bed         = pd.read_csv(holdout_bed_path, sep=\"\\t\", names=['chrom', 'start', 'end', 'label'], usecols=['chrom','start','end'])\n",
    "\n",
    "    # Keep only a certain num of exmaples\n",
    "    if holdout_type == \"val\":\n",
    "        holdout_bed = holdout_bed.iloc[:1000000]\n",
    "    elif holdout_type == \"test\":\n",
    "        holdout_bed = holdout_bed.iloc[:2000000]\n",
    "\n",
    "    # Get agreement sites so we can subset\n",
    "    agreement_sites = get_agreement_sites(params=params, verbose=verbose)\n",
    "    holdout_bed     = holdout_bed.iloc[list(agreement_sites[params.tf][\"TP\"])]\n",
    "\n",
    "    # Randomly subsample 2,000 sites from the val or test beds\n",
    "    shuffled_indices = np.random.permutation(len(holdout_bed))[:2000]\n",
    "    holdout_bed = holdout_bed.iloc[shuffled_indices]\n",
    "\n",
    "    print(f\"> Currently we use all sites types to randomly subsample {len(holdout_bed)} values\\n\")\n",
    "\n",
    "    # Create the necessary strings for the regions we need\n",
    "    chrom_values    = holdout_bed.iloc[:, 0]\n",
    "    start_values    = holdout_bed.iloc[:, 1]\n",
    "    end_values      = holdout_bed.iloc[:, 2]\n",
    "\n",
    "    print(f\"> Creating interval regions to use...\\n\")\n",
    "\n",
    "    # Create the region strings\n",
    "    regions = [\n",
    "        f\"{chrom}:{start}-{end}\"\n",
    "        for chrom, start, end in zip(chrom_values, start_values, end_values)\n",
    "    ]\n",
    "\n",
    "    shuffled_indices = np.random.permutation(len(regions)) # [:2000]\n",
    "    regions = [regions[i] for i in shuffled_indices]\n",
    "\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crested_contributions(intervals, model, genome, target_idx=None):\n",
    "    \"\"\"\n",
    "    Computes contribution scores via expected integrated gradients!\n",
    "    We use the `crested` library to do it :) I really like it!\n",
    "\n",
    "    Args:\n",
    "        intervals: chromosomal regions to use for analysis\n",
    "        model: The (TensorFlow/PyTorch) model\n",
    "        genome: The species to get chromosomal regions from\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of attribution scores.\n",
    "    \"\"\"\n",
    "\n",
    "    scores, one_hot_encoded_sequences = contribution_scores(\n",
    "        input=intervals,\n",
    "        target_idx=target_idx,\n",
    "        model=model,\n",
    "        method='integrated_grad',\n",
    "        genome=GENOMES[genome],\n",
    "        transpose=True,\n",
    "        output_dir=None,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return scores, one_hot_encoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_holdout_bed(holdout_type, params, verbose=True):\n",
    "\n",
    "#     from seqdataloader.batchproducers.coordbased.core import Coordinates\n",
    "#     from seqdataloader.batchproducers.coordbased.coordstovals.fasta import PyfaidxCoordsToVals\n",
    "\n",
    "#     assert holdout_type in [\"test\", \"val\"], f\"Invalid holdout type {holdout_type}. Please choose from ['test', 'val']\"\n",
    "\n",
    "#     print(f\"> Converting sites from the {holdout_type} set so that we can use them to get model attributions.\\n\")\n",
    "\n",
    "#     holdout_bed_path    = f\"{ROOT}/data/{params.target_species}/{params.tf}/{holdout_type}_shuf.bed\"\n",
    "#     converter           = PyfaidxCoordsToVals(params.genome_files[params.target_species])\n",
    "\n",
    "#     # Keep only a certain num of exmaples\n",
    "#     if holdout_type == \"val\":\n",
    "#         coords_tmp\t= [line.split() for line in open(holdout_bed_path)][:1000000]\n",
    "#     elif holdout_type == \"test\":\n",
    "#         coords_tmp\t= [line.split() for line in open(holdout_bed_path)][:2000000]\n",
    "\n",
    "#     # Subset based on the extreme agreement sites\n",
    "#     agreement_sites = get_agreement_sites(params=params, verbose=verbose)\n",
    "    \n",
    "#     print(f\"> Currently we keep all sites no matter the sigmoid value...\\n\")\n",
    "\n",
    "#     coords = [Coordinates(coord[0], int(coord[1]), int(coord[2])) for coord in coords_tmp]\n",
    "\n",
    "#     print(f\"> Converting {len(coords)} coordinates to ohes...\\n\")\n",
    "\n",
    "#     return converter(coords).transpose((0,2,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_on_seqs(\n",
    "#     model,\n",
    "#     genome,\n",
    "#     x: Union[str, List[str]],\n",
    "#     device: Union[str, int] = \"cpu\",\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     A simple function to return model predictions directly\n",
    "#     on a batch of a single batch of sequences in string\n",
    "#     format.\n",
    "\n",
    "#     Args:\n",
    "#         x: DNA sequences as a string or list of strings.\n",
    "#         device: Index of the device to use\n",
    "\n",
    "#     Returns:\n",
    "#         A numpy array of predictions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Handle (assumed) interval input\n",
    "#     import grelu.sequence.format\n",
    "\n",
    "#     input_seqs = grelu.sequence.format.convert_input_type(\n",
    "#         x,\n",
    "#         output_type=\"one_hot\",\n",
    "#         genome=genome,\n",
    "#         add_batch_axis=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     model = model.eval().to(device)\n",
    "#     preds = model.forward(input_seqs).detach().cpu().numpy()\n",
    "#     model = model.cpu()\n",
    "#     return preds\n",
    "\n",
    "# def ISM_predict(\n",
    "#     seqs: Union[pd.DataFrame, np.ndarray, str, List[str]],\n",
    "#     model: Callable,\n",
    "#     genome: Optional[str] = None,\n",
    "#     prediction_transform: Optional[Callable] = None,\n",
    "#     start_pos: int = 0,\n",
    "#     end_pos: Optional[int] = None,\n",
    "#     compare_func: Optional[Union[str, Callable]] = None,\n",
    "#     devices: Union[str, List[int]] = \"cpu\",\n",
    "#     num_workers: int = 1,\n",
    "#     batch_size: int = 64,\n",
    "#     return_df: bool = True,\n",
    "# ) -> Union[np.array, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Predicts the importance scores of each nucleotide position in a given DNA sequence\n",
    "#     using the In Silico Mutagenesis (ISM) method.\n",
    "\n",
    "#     Args:\n",
    "#         seqs: Input DNA sequences as genomic intervals, strings, or integer-encoded form.\n",
    "#         genome: Name of the genome to use if a genomic interval is supplied.\n",
    "#         model: A pre-trained deep learning model\n",
    "#         prediction_transform: A module to transform the model output\n",
    "#         start_pos: Index of the position to start applying ISM\n",
    "#         end_pos: Index of the position to stop applying ISM\n",
    "#         compare_func: A function or name of a function to compare the predictions for mutated\n",
    "#             and reference sequences. Allowed names are \"divide\", \"subtract\" and \"log2FC\".\n",
    "#             If not provided, the raw predictions for both mutant and reference sequences will\n",
    "#             be returned.\n",
    "#         devices: Indices of the devices on which to run inference\n",
    "#         num_workers: number of workers for inference\n",
    "#         batch_size: batch size for model inference\n",
    "#         return_df: If True, the ISM results will be returned as a dataframe. Otherwise, they\n",
    "#             will be returned as a Numpy array.\n",
    "\n",
    "#     Returns:\n",
    "#         A numpy array of the predicted scores for each nucleotide position (if return_df = False)\n",
    "#         or a pandas dataframe with A, C, G, and T as row labels and the bases at each position\n",
    "#         of the sequence as column labels  (if return_df = True).\n",
    "#     \"\"\"\n",
    "#     from grelu.data.dataset import ISMDataset\n",
    "#     from grelu.sequence.format import BASE_TO_INDEX_HASH, STANDARD_BASES\n",
    "#     from grelu.sequence.utils import get_unique_length\n",
    "#     from grelu.utils import get_compare_func, make_list\n",
    "\n",
    "#     # Get sequence as string\n",
    "#     seqs = convert_input_type(seqs, \"strings\", genome=genome)\n",
    "#     seqs = make_list(seqs)\n",
    "\n",
    "#     # Get the last position to mutate\n",
    "#     if end_pos is None:\n",
    "#         end_pos = get_unique_length(seqs)\n",
    "\n",
    "#     # Make dataset\n",
    "#     ism = ISMDataset(\n",
    "#         seqs=seqs,\n",
    "#         positions=range(start_pos, end_pos),\n",
    "#         drop_ref=False,\n",
    "#     )\n",
    "\n",
    "#     # Add transform to model\n",
    "#     model.add_transform(prediction_transform)\n",
    "\n",
    "#     # Get predictions for all mutated sequences\n",
    "#     preds = model.predict_on_dataset(\n",
    "#         ism,\n",
    "#         devices=devices,\n",
    "#         num_workers=num_workers,\n",
    "#         batch_size=batch_size,\n",
    "#     )\n",
    "#     # B, L, 4, T, L\n",
    "\n",
    "#     if compare_func is not None:\n",
    "\n",
    "#         # Slice the prediction corresponding to each reference sequence\n",
    "#         ref_bases = [BASE_TO_INDEX_HASH[seq[start_pos]] for seq in seqs]\n",
    "#         ref_preds = np.concatenate(\n",
    "#             [preds[None, None, None, i, 0, x] for i, x in enumerate(ref_bases)]\n",
    "#         )  # B, L, 1, T, L\n",
    "\n",
    "#         # Compare all predictions to the prediction for the corresponding reference sequence\n",
    "#         preds = get_compare_func(compare_func, tensor=False)(preds, ref_preds)\n",
    "\n",
    "#     # Convert into a dataframe\n",
    "#     if return_df:\n",
    "#         if (preds.shape[0] == 1) and (preds.shape[3:] == (1, 1)):\n",
    "#             preds = preds.squeeze(axis=(0, 3, 4))  # L, 4\n",
    "#             preds = pd.DataFrame(\n",
    "#                 preds.T,  # 4, L\n",
    "#                 index=STANDARD_BASES,\n",
    "#                 columns=[b for b in seqs[0][start_pos:end_pos]],\n",
    "#             )\n",
    "#         else:\n",
    "#             warnings.warn(\n",
    "#                 \"Cannot return a dataframe as either multiple sequences are \\\n",
    "#                 supplied or the model predictions are multi-dimensional. Returning Numpy array.\"\n",
    "#             )\n",
    "\n",
    "#     # Remove transform\n",
    "#     model.reset_transform()\n",
    "#     return preds\n",
    "\n",
    "# def get_attributions(\n",
    "#     model,\n",
    "#     seqs: Union[pd.DataFrame, np.array, List[str]],\n",
    "#     genome: Optional[str] = None,\n",
    "#     prediction_transform: Optional[Callable] = None,\n",
    "#     device: Union[str, int] = \"cpu\",\n",
    "#     method: str = \"deepshap\",\n",
    "#     hypothetical: bool = False,\n",
    "#     n_shuffles: int = 20,\n",
    "#     seed=None,\n",
    "#     **kwargs,\n",
    "# ) -> np.array:\n",
    "#     \"\"\"\n",
    "#     Get per-nucleotide importance scores for sequences using Captum.\n",
    "\n",
    "#     Args:\n",
    "#         model: A trained deep learning model\n",
    "#         seqs: input DNA sequences as genomic intervals, strings, or integer-encoded form.\n",
    "#         genome: Name of the genome to use if a genomic interval is supplied.\n",
    "#         prediction_transform: A module to transform the model output\n",
    "#         devices: Indices of the devices to use for inference\n",
    "#         method: One of \"deepshap\", \"saliency\", \"inputxgradient\" or \"integratedgradients\"\n",
    "#         hypothetical: whether to calculate hypothetical importance scores.\n",
    "#             Set this to True to obtain input for tf-modisco, False otherwise\n",
    "#         n_shuffles: Number of times to dinucleotide shuffle sequence\n",
    "#         seed: Random seed\n",
    "#         **kwargs: Additional arguments to pass to tangermeme.deep_lift_shap.deep_lift_shap\n",
    "\n",
    "#     Returns:\n",
    "#         Per-nucleotide importance scores as numpy array of shape (B, 4, L).\n",
    "#     \"\"\"\n",
    "#     # One-hot encode the input\n",
    "#     seqs = convert_input_type(seqs, \"one_hot\", genome=genome, add_batch_axis=True)\n",
    "\n",
    "#     # Add transform to model\n",
    "#     model.add_transform(prediction_transform)\n",
    "#     model = model.eval()\n",
    "\n",
    "#     # Empty list for the output\n",
    "#     attributions = []\n",
    "\n",
    "#     # Check hypothetical\n",
    "#     if hypothetical:\n",
    "#         if method != \"deepshap\":\n",
    "#             warnings.warn(\n",
    "#                 \"hypothetical = True will be ignored as method is not deepshap.\"\n",
    "#             )\n",
    "\n",
    "#     # Initialize the attributer\n",
    "#     if method == \"deepshap\":\n",
    "#         if isinstance(model.model, EnformerModel) or isinstance(\n",
    "#             model.model, EnformerPretrainedModel\n",
    "#         ):\n",
    "#             raise NotImplementedError(\n",
    "#                 \"DeepShap currently cannot be applied to Enformer models.\"\n",
    "#             )\n",
    "#         else:\n",
    "#             attributions = deep_lift_shap(\n",
    "#                 model,\n",
    "#                 X=seqs,\n",
    "#                 n_shuffles=n_shuffles,\n",
    "#                 hypothetical=hypothetical,\n",
    "#                 device=device,\n",
    "#                 random_state=seed,\n",
    "#                 **kwargs,\n",
    "#             ).numpy(force=True)\n",
    "\n",
    "#     else:\n",
    "#         if method == \"integratedgradients\":\n",
    "#             attributer = IntegratedGradients(model.to(device))\n",
    "#         elif method == \"inputxgradient\":\n",
    "#             attributer = InputXGradient(model.to(device))\n",
    "#         elif method == \"saliency\":\n",
    "#             attributer = Saliency(model.to(device))\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#         # Calculate attributions for each sequence\n",
    "#         with torch.no_grad():\n",
    "#             for i in range(len(seqs)):\n",
    "#                 X_ = seqs[i : i + 1].to(device)  # 1, 4, L\n",
    "#                 attr = attributer.attribute(X_)\n",
    "#                 attributions.append(attr.cpu().numpy())\n",
    "\n",
    "#         attributions = np.vstack(attributions)\n",
    "\n",
    "#     # Remove transform\n",
    "#     model.reset_transform()\n",
    "#     return attributions  # N, 4, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBaseline(keras.Model):\n",
    "    def __init__(self, model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model    = TorchModuleWrapper(model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class KerasEvo(keras.Model):\n",
    "    def __init__(self, fe_model, cls_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fe_model    = TorchModuleWrapper(fe_model)\n",
    "        self.cls_model   = TorchModuleWrapper(cls_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fe_model(x)\n",
    "        x = self.cls_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE        = True\n",
    "\n",
    "seed        = 1182024\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "# Make compatible, the model, with attribution and set to eval mode\n",
    "# https://discuss.pytorch.org/t/when-should-we-set-torch-backends-cudnn-enabled-to-false-especially-for-lstm/106571/5\n",
    "#basic_model.eval().to(device)\n",
    "#basic_model.gru_tower.train()\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # Instantiate the model\n",
    "# basic_model = BasicModel(params)\n",
    "# #basic_model.to(device) # Move to device\n",
    "# #basic_model.eval()     # Set eval mode\n",
    "\n",
    "# print(f\"Loading model from {model_file}\\n\")\n",
    "\n",
    "# # Load the state dict from the file\n",
    "# state_dict = torch.load(model_file, map_location=device)\n",
    "\n",
    "# # Create a new state dict inserting '.module' after the first component\n",
    "# new_state_dict = OrderedDict()\n",
    "\n",
    "# for k, v in state_dict.items():\n",
    "#     # Find the first dot\n",
    "#     dot_index = k.find('.')\n",
    "#     if dot_index != -1:\n",
    "#         # Split the key into the first part and the rest (including the dot)\n",
    "#         part1 = k[:dot_index]\n",
    "#         part2 = k[dot_index:] # Starts with '.'\n",
    "#         # Construct the new key by inserting '.module'\n",
    "#         new_key = part1 + '.module' + part2\n",
    "#         new_state_dict[new_key] = v\n",
    "#     else:\n",
    "#         # If there's no dot (e.g., a parameter directly on BasicModel),\n",
    "#         # it's unclear how to transform it based on the error pattern.\n",
    "#         # Keep it as is for now, or decide on a rule if such keys exist.\n",
    "#         # Based on your error, all relevant keys seem to have dots.\n",
    "#         print(f\"Warning: Key '{k}' does not contain '.' and was not modified.\")\n",
    "#         new_state_dict[k] = v # Keep original key if no dot is found\n",
    "\n",
    "\n",
    "# # Load the corrected state dict\n",
    "# try:\n",
    "#     basic_model.load_state_dict(new_state_dict)\n",
    "#     print(\"Model loaded successfully!\")\n",
    "# except RuntimeError as e:\n",
    "#     print(f\"Error loading modified state_dict: {e}\")\n",
    "#     print(\"Double-check if the transformation rule is correct for all keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create the basic model and load saved weights from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on CEBPA, tested on mm10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper            │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_1          │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper            │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_1          │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m21:39:50\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model:   0%|          | 0/1 [00:00<?, ?it/s]/net/talisker/home/benos/mae117/.conda/envs/crested/lib/python3.10/site-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n",
      "Model: 100%|██████████| 1/1 [10:34<00:00, 634.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on FOXA1, tested on mm10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_2          │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_3          │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_2          │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_3          │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m21:51:06\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [11:39<00:00, 699.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF4A, tested on mm10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_4          │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_5          │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_4          │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_5          │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m22:03:30\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:24<00:00, 624.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF6, tested on mm10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_6          │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_7          │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_6          │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_7          │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m22:14:36\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [11:29<00:00, 689.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on CEBPA, tested on rheMac10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_8          │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_9          │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_8          │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_9          │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m22:26:53\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:24<00:00, 624.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on FOXA1, tested on rheMac10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_10         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_11         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_10         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_11         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 1897 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m22:38:04\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 1897 region(s).\n",
      "Model: 100%|██████████| 1/1 [09:53<00:00, 593.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF4A, tested on rheMac10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_12         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_13         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_12         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_13         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m22:48:45\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:25<00:00, 626.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF6, tested on rheMac10 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_14         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_15         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_14         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_15         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m22:59:58\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:27<00:00, 627.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on CEBPA, tested on canFam6 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_16         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_17         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_16         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_17         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m23:11:09\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:25<00:00, 625.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on FOXA1, tested on canFam6 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_18         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_19         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_18         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_19         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 1894 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m23:22:16\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 1894 region(s).\n",
      "Model: 100%|██████████| 1/1 [09:52<00:00, 592.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF4A, tested on canFam6 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_20         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_21         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_20         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_21         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m23:32:52\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:25<00:00, 625.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF6, tested on canFam6 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_22         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_23         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_22         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_23         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m23:44:00\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:25<00:00, 625.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on CEBPA, tested on rn7 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_24         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_25         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_24         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_25         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m23:55:08\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:26<00:00, 626.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on FOXA1, tested on rn7 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_26         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_27         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_26         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_27         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:06:18\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:29<00:00, 629.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF4A, tested on rn7 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_28         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_29         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_28         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_29         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:17:31\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:28<00:00, 628.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attributing MORALE on HNF6, tested on rn7 ---\n",
      "\n",
      "> Loading from saved models files...\n",
      "\n",
      "\t>> Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_evo_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_evo_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_30         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">960,303</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_31         │ ?                      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_30         │ ?                      │       \u001b[38;5;34m960,303\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ torch_module_wrapper_31         │ ?                      │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960,367</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m960,367\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "> Converting sites from the test set so that we can use them to get model attributions.\n",
      "\n",
      "> Currently we use all sites types to randomly subsample 2000 values\n",
      "\n",
      "> Creating interval regions to use...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:28:41\u001b[0m \u001b[1m|\u001b[0m \u001b[34mINFO\u001b[0m \u001b[1m|\u001b[0m Calculating contribution scores for 1 class(es) and 2000 region(s).\n",
      "Model: 100%|██████████| 1/1 [10:26<00:00, 626.29s/it]\n"
     ]
    }
   ],
   "source": [
    "for target_species in SPECIES:\n",
    "    for model_name in [\"MORALE\"]:\n",
    "        for tf in TFS:\n",
    "\n",
    "            print(f\"\\n--- Attributing {model_name} on {tf}, tested on {target_species} ---\\n\")\n",
    "\n",
    "            # (1) Define new params set\n",
    "\n",
    "            params = Params(args = [\"Attribtuion\", tf, target_species], verbose=False)\n",
    "\n",
    "            # (2) Load in the model we are calling and their saved weights\n",
    "            \n",
    "            print(f\"> Loading from saved models files...\\n\")\n",
    "\n",
    "            if model_name == \"Baseline\":\n",
    "                model_file = get_model_file(model=model_name, params=params)\n",
    "\n",
    "                baseline_model = BaselineModel(params)\n",
    "                baseline_model.load_state_dict(torch.load(model_file))\n",
    "                \n",
    "                keras_model = KerasBaseline(model=baseline_model).eval().to('cpu')\n",
    "\n",
    "                print(f\"\\t>> Model summary:\\n\")\n",
    "                print(keras_model.summary())\n",
    "\n",
    "            else:\n",
    "                fe_file, cls_file   = get_model_file(model=model_name, params=params)\n",
    "\n",
    "                feature_extractor = FeatureExtractor(params)\n",
    "                feature_extractor.load_state_dict(torch.load(fe_file))\n",
    "\n",
    "                classifier = Classifier(params)\n",
    "                classifier.load_state_dict(torch.load(cls_file))\n",
    "\n",
    "                keras_model = KerasEvo(fe_model=feature_extractor, cls_model=classifier).eval().to('cpu')\n",
    "\n",
    "                print(f\"\\t>> Model summary:\")\n",
    "                print(keras_model.summary())\n",
    "                print(\"\\n\")\n",
    "\n",
    "            # (3) Create data\n",
    "            regions = read_holdout_bed(holdout_type=\"test\", params=params, verbose=False)\n",
    "\n",
    "            # (4) Compute scores via IG and save\n",
    "            scores, seqs = get_crested_contributions(\n",
    "                intervals=regions,\n",
    "                target_idx=0,\n",
    "                model=keras_model,\n",
    "                genome=params.target_species\n",
    "            )\n",
    "\n",
    "            # TF-Modisco assumes a length-last format\n",
    "            if SAVE:\n",
    "                np.save(f\"{ROOT}/plots/crested/{model_name}_{tf}_{target_species}_scores.npy\", scores.squeeze())\n",
    "                np.save(f\"{ROOT}/plots/crested/{model_name}_{tf}_{target_species}_seqs.npy\", seqs.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crested",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
