{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries, define constants, functions, and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../2_train_and_test_models\")\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from params import ROOT, GENOMES, TFS, SPECIES, Params\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, confusion_matrix, log_loss, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorthand names for all model types to include in plots\n",
    "MODELS = [\n",
    "    \"BM\",\n",
    "    \"GRL\",\n",
    "    \"MORALE\"\n",
    "]\n",
    "\n",
    "# Plot-acceptable names for model types\n",
    "MODEL_NAMES = {\n",
    "    \"BM-mm10\": \"Mouse-trained\",\n",
    "    \"BM-hg38\": \"Human-trained\",\n",
    "    \"GRL-mm10\": \"Mouse-trained (+GRL)\",\n",
    "    \"GRL-hg38\": \"Human-trained (+GRL)\",\n",
    "    \"MORALE-mm10\": \"Mouse-trained (+MORALE)\",\n",
    "    \"MORALE-hg38\": \"Human-trained (+MORALE)\"\n",
    "}\n",
    "\n",
    "# For simplicity, we will only consider the following repeat types\n",
    "REPEAT_TYPES = [\"LINE\", \"SINE\"]\n",
    "\n",
    "# if (test_species == \"mm10\"):\n",
    "#     REPEAT_TYPES = [\"DNA\", \"LINE\", \"Low_complexity\", \"LTR\", \"Other\", \"Satellite\", \"Simple_repeat\", \"SINE\", \"Unknown\"]\n",
    "#     # Removed due to < 500 instances in the test set: [\"RC\"]\n",
    "# elif (test_species == \"hg38\"):\n",
    "#     REPEAT_TYPES = [\"DNA\", \"LINE\", \"Low_complexity\", \"LTR\", \"Simple_repeat\", \"SINE\", \"Unknown\"]\n",
    "#     # Removed due to < 500 instances in the test set: [\"RC\", \"Retroposon\", \"RNA\", \"rRNA\", \"Satellite\", \"scRNA\", \"srpRNA\", \"tRNA\"]\n",
    "# else:\n",
    "#     raise ValueError(\"Invalid test species\")\n",
    "\n",
    "SPECIES1 = \"hg38\"\n",
    "SPECIES2 = \"mm10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_file(model, tf, source_species, domain):\n",
    "    preds_root = f\"{ROOT}/output\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/{model}_tf-{tf}_trained-{source_species}_tested-{domain}.preds.npy\"\n",
    "\n",
    "def get_labels_file(model, tf, source_species, domain):\n",
    "    preds_root = f\"{ROOT}/output\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/{model}_tf-{tf}_trained-{source_species}_tested-{domain}.labels.npy\"\n",
    "\n",
    "def load_fivefold_data(average=False, verbose=False):\n",
    "    preds_dict      = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    labels_dict     = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    bound_indices   = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    unbound_indices = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "\n",
    "    # Loop over mouse-trained, human-trained models, and domain-adaptive models\n",
    "    for model in MODELS:\n",
    "        for tf in TFS:\n",
    "            for source in SPECIES:\n",
    "                for target in SPECIES:\n",
    "                    if verbose:\n",
    "                        print(f\"\\t({model} on {tf} when: {source}-trained, and {target}-tested)\")\n",
    "                        \n",
    "                    preds_file  = get_preds_file(model=model, tf=tf, source_species=source, domain=target)\n",
    "                    labels_file = get_labels_file(model=model, tf=tf, source_species=source, domain=target)\n",
    "                \n",
    "                    try:\n",
    "                        # Load them\n",
    "                        preds = np.load(preds_file)\n",
    "                        labels = np.load(labels_file)\n",
    "\n",
    "                        # Calculate if we need to truncate the labels\n",
    "                        if preds.shape[0] != labels.shape[0]:\n",
    "                            print(\"\\nTruncating labels\\n\")\n",
    "                            labels = labels[:preds.shape[0]]\n",
    "\n",
    "                        assert preds.shape[0] == labels.shape[0]\n",
    "\n",
    "                        if average:\n",
    "                            # We take the average of the sigmoid values across the five-folds\n",
    "                            # to determine the confusion matrix\n",
    "                            preds_dict[f\"{model}-{source}\"][tf][target] = np.mean(preds, axis=1)\n",
    "                        else:                        \n",
    "                            # We save predictions from each of the five-folds per model, TF, source, and target\n",
    "                            preds_dict[f\"{model}-{source}\"][tf][target] = np.load(preds_file)\n",
    "                        \n",
    "                        labels_dict[f\"{model}-{source}\"][tf][target] = np.load(labels_file)\n",
    "\n",
    "                        # Store unbound and bound indices for all models, TFs, sources, and targets\n",
    "                        bound_indices[f\"{model}-{source}\"][tf][target]      = np.where(labels == 1)[0]\n",
    "                        unbound_indices[f\"{model}-{source}\"][tf][target]    = np.where(labels == 0)[0]\n",
    "                        \n",
    "                    except:\n",
    "                        print(\"Could not load regular preds/labels files\")\n",
    "\n",
    "    return preds_dict, labels_dict, bound_indices, unbound_indices\n",
    "\n",
    "def generate_confusion_matrix(preds_dict, labels_dict, percents=False, differential=False, performance=False):\n",
    "    # This function generates the full confusion matrix over predicitions for all models\n",
    "    # that we care about. Additionally, we include the RAW number of differential predictions\n",
    "    # (only errors, so type 1 or 2).\n",
    "\n",
    "    #cnf_matrix = dict()\n",
    "    cnf_matrix = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "\n",
    "    # now go through each model and tf and calculate confusion matrix\n",
    "    for model in MODELS:\n",
    "        adapted_model_name                  = f\"{model}-{SPECIES1}\"\n",
    "        ground_truth_model_name             = f\"{model}-{SPECIES2}\"\n",
    "\n",
    "        for tf in TFS:\n",
    "            for the_bound in [adapted_model_name, ground_truth_model_name]:\n",
    "\n",
    "                if \"BM\" not in model and the_bound == ground_truth_model_name:\n",
    "                    continue\n",
    "                else:\n",
    "                    cnf_matrix[the_bound][tf] = {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}\n",
    "\n",
    "                    # (0) First we just calculate the raw confusion matrix for the model\n",
    "                    preds   = preds_dict[the_bound][tf][SPECIES2]\n",
    "                    labels  = labels_dict[the_bound][tf][SPECIES2]\n",
    "\n",
    "                    # We need to categorize the adapted predictions based on the labels\n",
    "                    tp_preds = preds[(preds > 0.5) & (labels == 1)]\n",
    "                    fp_preds = preds[(preds > 0.5) & (labels == 0)]\n",
    "                    tn_preds = preds[(preds <= 0.5) & (labels == 0)]\n",
    "                    fn_preds = preds[(preds <= 0.5) & (labels == 1)]\n",
    "                    \n",
    "                    # (1) Now we add the raw counts for each category to the confusion matrix\n",
    "                    if percents:\n",
    "                        cnf_matrix[the_bound][tf][\"TP\"] = round((tp_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                        cnf_matrix[the_bound][tf][\"FP\"] = round((fp_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                        cnf_matrix[the_bound][tf][\"TN\"] = round((tn_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                        cnf_matrix[the_bound][tf][\"FN\"] = round((fn_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                    else:\n",
    "                        cnf_matrix[the_bound][tf][\"TP\"] = tp_preds.shape[0]\n",
    "                        cnf_matrix[the_bound][tf][\"FP\"] = fp_preds.shape[0]\n",
    "                        cnf_matrix[the_bound][tf][\"TN\"] = tn_preds.shape[0]\n",
    "                        cnf_matrix[the_bound][tf][\"FN\"] = fn_preds.shape[0]\n",
    "\n",
    "                    # (2) We are primarily interested in gauging the differential false positive \n",
    "                    # predictions between the target model to the source model\n",
    "                    if differential:\n",
    "\n",
    "                        # Adapted models overpredict as compared to the ground truth model\n",
    "                        overpred        = set(np.nonzero(preds - preds_dict[ground_truth_model_name][tf][SPECIES2] > 0.5)[0])\n",
    "                        #overpred        = set(np.nonzero(preds - preds_dict[\"BM-mm10\"][tf][SPECIES2] > 0.5)[0])\n",
    "                        overpred_sites  = np.array([False if i not in overpred else True for i, j in enumerate(labels)])\n",
    "\n",
    "                        # Adapted models underpredict as compared to the ground truth model\n",
    "                        underpred       = set(np.nonzero(preds_dict[ground_truth_model_name][tf][SPECIES2] - preds > 0.5)[0])\n",
    "                        #underpred       = set(np.nonzero(preds_dict[\"BM-mm10\"][tf][SPECIES2] - preds > 0.5)[0])\n",
    "                        underpred_list  = np.array([False if i not in underpred else True for i, j in enumerate(labels)])\n",
    "\n",
    "                        if percents:\n",
    "                            cnf_matrix[the_bound][tf]['dTP'] = round((preds[(preds > 0.5) & (labels == 1) & (overpred_sites == True)].shape[0] / tp_preds.shape[0]) * 100, 3)\n",
    "                            cnf_matrix[the_bound][tf]['dFP'] = round((preds[(preds > 0.5) & (labels == 0) & (overpred_sites == True)].shape[0] / fp_preds.shape[0]) * 100, 3)\n",
    "                            cnf_matrix[the_bound][tf]['dTN'] = round((preds[(preds <= 0.5) & (labels == 0) & (underpred_list == True)].shape[0] / tn_preds.shape[0]) * 100, 3)\n",
    "                            cnf_matrix[the_bound][tf]['dFN'] = round((preds[(preds <= 0.5) & (labels == 1) & (underpred_list == True)].shape[0] / fn_preds.shape[0]) * 100, 3)\n",
    "                        else:\n",
    "                            cnf_matrix[the_bound][tf]['dTP'] = preds[(preds > 0.5) & (labels == 1) & (overpred_sites == True)].shape[0] / tp_preds.shape[0]\n",
    "                            cnf_matrix[the_bound][tf]['dFP'] = preds[(preds > 0.5) & (labels == 0) & (overpred_sites == True)].shape[0] / fp_preds.shape[0]\n",
    "                            cnf_matrix[the_bound][tf]['dTN'] = preds[(preds <= 0.5) & (labels == 0) & (underpred_list == True)].shape[0] / tn_preds.shape[0]\n",
    "                            cnf_matrix[the_bound][tf]['dFN'] = preds[(preds <= 0.5) & (labels == 1) & (underpred_list == True)].shape[0] / fn_preds.shape[0]\n",
    "\n",
    "                    # (3) We either get the performance metrics or we don't.\n",
    "                    if performance:\n",
    "                        \n",
    "                        # We cannot just grab the auPRC from the confusion matrix calcs, because\n",
    "                        # we averaged the sigmoid values over the five-folds. We need to load in the \n",
    "                        # performance data and do it manually.\n",
    "                        performance_df = pd.read_csv(f\"{ROOT}/plots/Tables1-2/performance_data.csv\", index_col=None)\n",
    "                        performance_df = performance_df.iloc[:, 1:]\n",
    "                        \n",
    "                        # We need to only grab the columns we need\n",
    "                        performance_model_name = MODEL_NAMES[the_bound]\n",
    "                        auPRC = performance_df.loc[performance_df[\"Model\"] == performance_model_name, :].loc[performance_df[\"Eval\"] == SPECIES2, :].loc[performance_df[\"TF\"] == tf, :].loc[:, \"auPRC\"]\n",
    "                        auPRC = round(np.mean(auPRC), 3)\n",
    "                        \n",
    "                        cnf_matrix[the_bound][tf]['auPRC'] = auPRC\n",
    "\n",
    "    return cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_bed_file(tf, species):\n",
    "    # This function returns the path to a BED-format file\n",
    "    # containing the chromosome names, starts, and ends for\n",
    "    # all examples to test the model with.\n",
    "    # Note this is specific to a TF (binding labels\n",
    "    # are loaded in from this file)!\n",
    "    return(f\"{ROOT}/data/{species}/{tf}/chr2.bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds_and_labels_dfs(preds_dict, labels_dict, repeat_labels):\n",
    "    preds_dfs = defaultdict(lambda : dict())\n",
    "\n",
    "    for model in MODELS:\n",
    "        adapted_model_name = f\"{model}-{SPECIES1}\"\n",
    "        for tf in TFS:\n",
    "            dict_to_make_into_df = {\"labels\" : labels_dict[adapted_model_name][tf][SPECIES2]}\n",
    "            goal_len = labels_dict[adapted_model_name][tf][SPECIES2].shape[0]  # assuming labels are already truncated\n",
    "\n",
    "            # Now we get the relevant predictions\n",
    "            model_preds = preds_dict[adapted_model_name][tf][SPECIES2]\n",
    "            assert model_preds.shape[0] == goal_len\n",
    "\n",
    "            dict_to_make_into_df[adapted_model_name] = model_preds\n",
    "\n",
    "            for repeat_type in REPEAT_TYPES:\n",
    "                dict_to_make_into_df[repeat_type] = repeat_labels[repeat_type][:goal_len]\n",
    "\n",
    "            preds_dfs[tf][adapted_model_name] = pd.DataFrame(dict_to_make_into_df)\n",
    "\n",
    "    return preds_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsk_file(test_species):\n",
    "    return ROOT + f\"/data/{test_species}/rmsk.bed\"\n",
    "\n",
    "def read_and_filter_rmsk_file(repeat_name, test_species, is_subfam=False, test_chrom=\"chr2\"):\n",
    "    # This function reads in the RepeatMasker bed file,\n",
    "    # filters for only rows listing annotations of one\n",
    "    # repeat type, and then returns only the start and \n",
    "    # end coordinate for each annotation.\n",
    "    \n",
    "    # We're assuming all the test set examples are on\n",
    "    # one chromosome, so we don't need the first column.\n",
    "    \n",
    "    # assuming bed format\n",
    "    filename = get_rmsk_file(test_species=test_species)\n",
    "    df = pd.read_csv(filename, sep = \"\\t\", usecols = [5, 6, 7, 10, 11], header = None)\n",
    "\n",
    "    if is_subfam:\n",
    "        df = df[df[10] == repeat_name]\n",
    "        df = df[df[5] == test_chrom]\n",
    "    else:\n",
    "        df = df[df[11] == repeat_name]\n",
    "        df = df[df[5] == test_chrom]\n",
    "\n",
    "    sorted_repeat_coords = sorted(list(zip(df[6], df[7])), key = lambda tup : tup[0])\n",
    "    return np.array(sorted_repeat_coords)\n",
    "\n",
    "def get_repeat_and_test_set_overlap(list_a, list_b):\n",
    "    # This function is similar to bedtools intersect,\n",
    "    # but returns a binary yes/no for overlap for each\n",
    "    # window in list_a.\n",
    "    \n",
    "    # Assumes everything's on the same chromosome\n",
    "    # Assumes inputs are lists of 2-ples: (start, stop) \n",
    "    \n",
    "    # output is list with len == len(list_a)\n",
    "    matches = []\n",
    "    b_index = 0\n",
    "    for a_item in list_a:\n",
    "        a_start, a_end = a_item\n",
    "        while True:\n",
    "            if b_index >= len(list_b):\n",
    "                matches.append(False)\n",
    "                break\n",
    "                \n",
    "            b_start, b_end = list_b[b_index]\n",
    "            # the -1 is because bed files are 1-indexed\n",
    "            if b_start > a_end - 1:  \n",
    "                matches.append(False)\n",
    "                break\n",
    "            elif b_end <= a_start:\n",
    "                b_index += 1\n",
    "            else:\n",
    "                matches.append(True)\n",
    "                break\n",
    "    assert len(matches) == len(list_a)\n",
    "    return np.array(matches)\n",
    "\n",
    "def get_test_bed_coords(test_species):\n",
    "    # This function loads in the bed file for the test set\n",
    "    # and keeps only the start and end coords for each entry.\n",
    "    # Here we assume the test set is 1 chromosome\n",
    "    \n",
    "    # later analysis will assume the coords are sorted,\n",
    "    # as in `sort -k1,1 -k2,2n $bed_file`\n",
    "    \n",
    "    # TF doesn't matter here because we're not using labels\n",
    "    test_bed = get_test_bed_file(tf=TFS[0], species=test_species)\n",
    "    df = pd.read_csv(test_bed, sep = \"\\t\", usecols = [1, 2], header = None)\n",
    "    return df.to_numpy()\n",
    "\n",
    "def get_all_repeat_labels_and_indices(test_species):\n",
    "    all_windows_coords  = get_test_bed_coords(test_species=test_species)\n",
    "    repeat_labels       = dict()\n",
    "    repeat_indices      = dict()\n",
    "\n",
    "    for repeat_type in REPEAT_TYPES:\n",
    "        \n",
    "        print(repeat_type)\n",
    "\n",
    "        repeat_type_coords = read_and_filter_rmsk_file(repeat_name=repeat_type, test_species=test_species)\n",
    "        # filtering for repeat types with at least 500 instances\n",
    "        # in the test set, so we don't get incorrectly extreme results\n",
    "\n",
    "        assert len(repeat_type_coords) > 500, (repeat_type, len(repeat_type_coords))\n",
    "\n",
    "        repeat_labels[repeat_type] = get_repeat_and_test_set_overlap(all_windows_coords, repeat_type_coords)\n",
    "        \n",
    "        repeat_indices[repeat_type] = set(np.nonzero(repeat_labels[repeat_type])[0])\n",
    "        \n",
    "    return repeat_labels, repeat_indices\n",
    "\n",
    "def fix_repeat_name(repeat_name):\n",
    "    # If a repeat name has an underscore in it, this will\n",
    "    # mess up the latex formatting, so we replace the\n",
    "    # underscores with spaces and then capitalize the first\n",
    "    # letter of each word in the repeat name\n",
    "    if \"_\" in repeat_name:\n",
    "        return \" \".join(repeat_name.split(\"_\")).title()\n",
    "    return repeat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repeat_table(preds_dfs, repeat_indices):\n",
    "    data_matrix = dict()\n",
    "    for model in MODELS:\n",
    "        adapted_model_name = f\"{model}-{SPECIES1}\"\n",
    "        data_matrix[adapted_model_name] = {i:{} for i in TFS}\n",
    "        for tf in TFS:\n",
    "\n",
    "            # (0) Instantiate an entry for each repeat type\n",
    "            data_matrix[adapted_model_name][tf] = {i:{} for i in REPEAT_TYPES}\n",
    "\n",
    "            # (0.5) Add a row for the eventual average across all repeat types\n",
    "            data_matrix[adapted_model_name][tf][\"Average\"] = {}\n",
    "\n",
    "            for rep in REPEAT_TYPES:\n",
    "\n",
    "                # (1) Get the indices of the repeat type in the test set\n",
    "                sites_with_repeat   = set([i for i, j in enumerate(preds_dfs[tf][adapted_model_name][rep])]).intersection(repeat_indices[rep])\n",
    "                probs_rep           = preds_dfs[tf][adapted_model_name].iloc[[i for i in sites_with_repeat], :]\n",
    "                preds_rep           = [1 if i > 0.5 else 0 for i in probs_rep[adapted_model_name]]\n",
    "                labels_rep          = preds_dfs[tf][adapted_model_name]['labels'].iloc[[i for i in sites_with_repeat]]\n",
    "\n",
    "                # (2) Now we use scikit to calculate different metrics for the model\n",
    "                #data_matrix[adapted_model_name][tf][rep][\"AUC\"] = roc_auc_score(y_true=labels_rep, y_score=preds_rep)\n",
    "                data_matrix[adapted_model_name][tf][rep][\"performance\"] = average_precision_score(y_true=labels_rep, y_score=preds_rep)\n",
    "\n",
    "            # (4) Now add the average for each metric across all repeat types\n",
    "            data_matrix[adapted_model_name][tf][\"Average\"][\"performance\"] = np.mean([data_matrix[adapted_model_name][tf][rep][\"performance\"] for rep in REPEAT_TYPES])\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "def print_table(preview_table, model_1, model_2, model_3, header=None, row_order=None, caption=None):\n",
    "    print(r'\\begin{table*}[t]{')\n",
    "    print(r'\\centering')\n",
    "\n",
    "    if caption is not None:\n",
    "        print(r'\\caption{' + caption + r'\\label{Tab:01}}')\n",
    "\n",
    "    print(r'\\resizebox{\\textwidth}{!}{')\n",
    "    print(r'\\setlength{\\tabcolsep}{0.8em}')\n",
    "\n",
    "    reps = 3\n",
    "    print(r'\\centering \\begin{tabular}{@{}ccccccccc@{}}\\toprule')\n",
    "\n",
    "    if header is None:\n",
    "        header = \"TF &\"\n",
    "        for i in REPEAT_TYPES:\n",
    "            if i != REPEAT_TYPES[-1]:\n",
    "                header += f\" {i} &\"\n",
    "            else:\n",
    "                header += f\" {i}\"\n",
    "        header += f\" & Average\"\n",
    "    \n",
    "    col_order = list(REPEAT_TYPES)  # Create a copy to avoid modifying the original list\n",
    "    col_order.extend([\"Average\"])\n",
    "\n",
    "    if row_order is None:\n",
    "        row_order = TFS\n",
    "    last_row = row_order[-1]\n",
    "\n",
    "    print(header + r' \\\\\\midrule')\n",
    "\n",
    "    table_segment = r'\\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source\\end{tabular}'  # Right after 1cm}, |>\n",
    "    line = r'& ' + ' & '.join([table_segment] * reps) + r' \\\\'  # Note the parentheses\n",
    "    print(line)\n",
    "\n",
    "    # Initialize a dictionary to store sums for each column and model\n",
    "    col_sums = {col: {model_1: 0, model_2: 0, model_3: 0} for col in col_order}\n",
    "\n",
    "    for row_key in row_order:\n",
    "        # Get model information\n",
    "        row_model1 = [preview_table[model_1][row_key][col] for col in col_order]\n",
    "        row_model2 = [preview_table[model_2][row_key][col] for col in col_order]\n",
    "        row_model3 = [preview_table[model_3][row_key][col] for col in col_order]\n",
    "\n",
    "        # Accumulate sums for averaging\n",
    "        for i, col in enumerate(col_order):\n",
    "            col_sums[col][model_1] += row_model1[i]['performance']\n",
    "            col_sums[col][model_2] += row_model2[i]['performance']\n",
    "            col_sums[col][model_3] += row_model3[i]['performance']\n",
    "        \n",
    "        # Find best model for each column\n",
    "        best_models = {}\n",
    "        for col_idx, col in enumerate(col_order):\n",
    "            values = [row_model1[col_idx]['performance'], row_model2[col_idx]['performance'], row_model3[col_idx]['performance']]\n",
    "            best_idx = np.argmax(values)\n",
    "            \n",
    "            if best_idx == 0:\n",
    "                best_models[col] = model_1\n",
    "            elif best_idx == 1:\n",
    "                best_models[col] = model_2\n",
    "            else:\n",
    "                best_models[col] = model_3\n",
    "\n",
    "        # Format model information, bolding the best\n",
    "        row_model1_as_str = [\n",
    "            (r\"\\textbf{\" + str(round(num['performance'], 3)) + r\"}\" if model_1 == best_models[col] else str(round(num['performance'], 3)))\n",
    "            for num, col in zip(row_model1, col_order)\n",
    "        ]\n",
    "        row_model2_as_str = [\n",
    "            (r\"\\textbf{\" + str(round(num['performance'], 3)) + r\"}\" if model_2 == best_models[col] else str(round(num['performance'], 3)))\n",
    "            for num, col in zip(row_model2, col_order)\n",
    "        ]\n",
    "        row_model3_as_str = [\n",
    "            (r\"\\textbf{\" + str(round(num['performance'], 3)) + r\"}\" if model_3 == best_models[col] else str(round(num['performance'], 3)))\n",
    "            for num, col in zip(row_model3, col_order)\n",
    "        ]\n",
    "\n",
    "        # Combine model information\n",
    "        combined_row = [\n",
    "            {model_1: i[0], model_2: i[1], model_3: i[2]}\n",
    "            for i in zip(row_model1_as_str, row_model2_as_str, row_model3_as_str)\n",
    "        ]\n",
    "        combine_row_as_str = [\n",
    "            r\"\\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}\"\n",
    "            + f\"{i[model_1]} & {i[model_2]} & {i[model_3]}\"\n",
    "            r\"\\end{tabular}\"\n",
    "            for i in combined_row\n",
    "        ]\n",
    "        tf_fancy_name = TFS[TFS.index(row_key)]\n",
    "        print(tf_fancy_name + \" & \" + \" & \".join(combine_row_as_str) + r' \\\\')\n",
    "\n",
    "    # Calculate and print the average row\n",
    "    avg_row = {}\n",
    "    num_rows = len(row_order)\n",
    "    for col in col_order:\n",
    "        avg_row[col] = {\n",
    "            model: round(col_sums[col][model] / num_rows, 3)\n",
    "            for model in [model_1, model_2, model_3]\n",
    "        }\n",
    "    \n",
    "    # Find the best performing model for each column in the average row\n",
    "    best_avg_models = {}\n",
    "    for col in col_order:\n",
    "        values = [avg_row[col][model_1], avg_row[col][model_2], avg_row[col][model_3]]\n",
    "        best_idx = np.argmax(values)\n",
    "            \n",
    "        if best_idx == 0:\n",
    "            best_avg_models[col] = model_1\n",
    "        elif best_idx == 1:\n",
    "            best_avg_models[col] = model_2\n",
    "        else:\n",
    "            best_avg_models[col] = model_3\n",
    "\n",
    "    # Format the average row as strings, bolding the best\n",
    "    avg_row_as_str = []\n",
    "    for col in col_order:\n",
    "        avg_row_as_str.append(\n",
    "            r\"\\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}\"\n",
    "            + (\n",
    "                r\"\\textbf{\"\n",
    "                + str(avg_row[col][model_1])\n",
    "                + r\"}\"\n",
    "                if model_1 == best_avg_models[col]\n",
    "                else str(avg_row[col][model_1])\n",
    "            )\n",
    "            + \" & \"\n",
    "            + (\n",
    "                r\"\\textbf{\"\n",
    "                + str(avg_row[col][model_2])\n",
    "                + r\"}\"\n",
    "                if model_2 == best_avg_models[col]\n",
    "                else str(avg_row[col][model_2])\n",
    "            )\n",
    "            + \" & \"\n",
    "            + (\n",
    "                r\"\\textbf{\"\n",
    "                + str(avg_row[col][model_3])\n",
    "                + r\"}\"\n",
    "                if model_3 == best_avg_models[col]\n",
    "                else str(avg_row[col][model_3])\n",
    "            )\n",
    "            + r\"\\end{tabular}\"\n",
    "        )\n",
    "    \n",
    "    print(r\"\\midrule\")\n",
    "    print(r\"Average & \" + \" & \".join(avg_row_as_str) + r' \\\\\\bottomrule')\n",
    "\n",
    "    print(r'\\end{tabular}}{}')\n",
    "    print(r'\\end{table*}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create repeat performance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE\n",
      "SINE\n"
     ]
    }
   ],
   "source": [
    "preds_dict, labels_dict, bound_indices, unbound_indices = load_fivefold_data(average=True, verbose=False)\n",
    "repeat_labels, repeat_indices                           = get_all_repeat_labels_and_indices(test_species=SPECIES2)\n",
    "preds_dfs                                               = make_preds_and_labels_dfs(preds_dict, labels_dict, repeat_labels)\n",
    "cnf_matrix                                              = generate_confusion_matrix(preds_dict, labels_dict, percents=True, differential=False, performance=True)\n",
    "repeat_table                                            = generate_repeat_table(preds_dfs, repeat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[t]{\n",
      "\\centering\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\setlength{\\tabcolsep}{0.8em}\n",
      "\\centering \\begin{tabular}{@{}ccccccccc@{}}\\toprule\n",
      "TF & LINE & SINE & Average \\\\\\midrule\n",
      "& \\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source\\end{tabular} & \\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source\\end{tabular} & \\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source\\end{tabular} \\\\\n",
      "CTCF & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.103 & \\textbf{0.124} & 0.111\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.102 & \\textbf{0.129} & 0.104\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.102 & \\textbf{0.126} & 0.107\\end{tabular} \\\\\n",
      "CEBPA & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.077 & 0.08 & \\textbf{0.082}\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.097 & 0.103 & \\textbf{0.108}\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.087 & 0.092 & \\textbf{0.095}\\end{tabular} \\\\\n",
      "HNF4A & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.04 & 0.043 & \\textbf{0.045}\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.061 & \\textbf{0.063} & 0.062\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.051 & \\textbf{0.053} & 0.053\\end{tabular} \\\\\n",
      "RXRA & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.027 & \\textbf{0.029} & 0.026\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.041 & \\textbf{0.042} & 0.035\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.034 & \\textbf{0.036} & 0.03\\end{tabular} \\\\\n",
      "\\midrule\n",
      "Average & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.062 & \\textbf{0.069} & 0.066\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.075 & \\textbf{0.084} & 0.077\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.068 & \\textbf{0.077} & 0.071\\end{tabular} \\\\\\bottomrule\n",
      "\\end{tabular}}{}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "print_table(repeat_table, model_1=f\"GRL-{SPECIES1}\", model_2=f\"MORALE-{SPECIES1}\", model_3=f\"BM-{SPECIES1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
