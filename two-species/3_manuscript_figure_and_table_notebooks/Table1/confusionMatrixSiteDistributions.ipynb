{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries, define constants, functions, and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../2_train_and_test_models\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.tri as tri\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from params import ROOT, GENOMES, TFS, SPECIES, Params\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorthand names for all model types to include in plots\n",
    "MODELS = [\n",
    "    \"BM\",\n",
    "    \"GRL\",\n",
    "    \"MORALE\"\n",
    "]\n",
    "\n",
    "# Plot-acceptable names for model types\n",
    "MODEL_NAMES = {\n",
    "    \"BM-mm10\": \"Mouse-trained\",\n",
    "    \"BM-hg38\": \"Human-trained\",\n",
    "    \"GRL-mm10\": \"Mouse-trained (+GRL)\",\n",
    "    \"GRL-hg38\": \"Human-trained (+GRL)\",\n",
    "    \"MORALE-mm10\": \"Mouse-trained (+MORALE)\",\n",
    "    \"MORALE-hg38\": \"Human-trained (+MORALE)\"\n",
    "}\n",
    "\n",
    "\n",
    "# Constants to be used for plot appearance details\n",
    "DOT_SIZE = 5\n",
    "\n",
    "ALPHA = 0.03\n",
    "AXIS_SIZE = 11\n",
    "AX_OFFSET = 0.02\n",
    "TF_TWINAX_OFFSET = 0.35\n",
    "FIG_SIZE_UNIT = 5\n",
    "FIG_SIZE_2_by_4 = (FIG_SIZE_UNIT, FIG_SIZE_UNIT * 2)\n",
    "FIG_SIZE_1_by_2 = (FIG_SIZE_UNIT / 2, FIG_SIZE_UNIT)\n",
    "BOUND_SUBSAMPLE_RATE = 4\n",
    "\n",
    "# If you don't care about plotting all examples\n",
    "# and want to speed things up, you can set SKIP to not None;\n",
    "# every SKIP-th ***UNBOUND*** example will be used in model evaluation.\n",
    "# Note that since bound sites are so sparse, SKIP only applies\n",
    "# to UNBOUND sites.\n",
    "SKIP = 200\n",
    "\n",
    "SPECIES1    = \"hg38\"\n",
    "SPECIES2    = \"mm10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_file(model, tf, source_species, domain):\n",
    "    preds_root = f\"{ROOT}/output\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/{model}_tf-{tf}_trained-{source_species}_tested-{domain}.preds.npy\"\n",
    "\n",
    "def get_labels_file(model, tf, source_species, domain):\n",
    "    preds_root = f\"{ROOT}/output\"\n",
    "    os.makedirs(preds_root, exist_ok=True)\n",
    "    return f\"{preds_root}/{model}_tf-{tf}_trained-{source_species}_tested-{domain}.labels.npy\"\n",
    "\n",
    "def load_fivefold_data(average=False, verbose=False):\n",
    "    preds_dict      = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    labels_dict     = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    bound_indices   = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "    unbound_indices = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "\n",
    "    # Loop over mouse-trained, human-trained models, and domain-adaptive models\n",
    "    for model in MODELS:\n",
    "        for tf in TFS:\n",
    "            for source in SPECIES:\n",
    "                for target in SPECIES:\n",
    "                    if verbose:\n",
    "                        print(f\"\\t({model} on {tf} when: {source}-trained, and {target}-tested)\")\n",
    "                        \n",
    "                    preds_file  = get_preds_file(model=model, tf=tf, source_species=source, domain=target)\n",
    "                    labels_file = get_labels_file(model=model, tf=tf, source_species=source, domain=target)\n",
    "                \n",
    "                    try:\n",
    "                        # Load them\n",
    "                        preds = np.load(preds_file)\n",
    "                        labels = np.load(labels_file)\n",
    "\n",
    "                        # Calculate if we need to truncate the labels\n",
    "                        if preds.shape[0] != labels.shape[0]:\n",
    "                            print(\"\\nTruncating labels\\n\")\n",
    "                            labels = labels[:preds.shape[0]]\n",
    "\n",
    "                        assert preds.shape[0] == labels.shape[0]\n",
    "\n",
    "                        if average:\n",
    "                            # We take the average of the sigmoid values across the five-folds\n",
    "                            # to determine the confusion matrix\n",
    "                            preds_dict[f\"{model}-{source}\"][tf][target] = np.mean(preds, axis=1)\n",
    "                        else:                        \n",
    "                            # We save predictions from each of the five-folds per model, TF, source, and target\n",
    "                            preds_dict[f\"{model}-{source}\"][tf][target] = np.load(preds_file)\n",
    "                        \n",
    "                        labels_dict[f\"{model}-{source}\"][tf][target] = np.load(labels_file)\n",
    "\n",
    "                        # Store unbound and bound indices for all models, TFs, sources, and targets\n",
    "                        bound_indices[f\"{model}-{source}\"][tf][target]      = np.where(labels == 1)[0]\n",
    "                        unbound_indices[f\"{model}-{source}\"][tf][target]    = np.where(labels == 0)[0]\n",
    "                        \n",
    "                    except:\n",
    "                        print(\"Could not load regular preds/labels files\")\n",
    "\n",
    "    return preds_dict, labels_dict, bound_indices, unbound_indices\n",
    "\n",
    "def generate_confusion_matrix(preds_dict, labels_dict, percents=False, differential=False, performance=False):\n",
    "    # This function generates the full confusion matrix over predicitions for all models\n",
    "    # that we care about. Additionally, we include the RAW number of differential predictions\n",
    "    # (only errors, so type 1 or 2).\n",
    "\n",
    "    #cnf_matrix = dict()\n",
    "    cnf_matrix = defaultdict(lambda : defaultdict(lambda : dict()))\n",
    "\n",
    "    # now go through each model and tf and calculate confusion matrix\n",
    "    for model in MODELS:\n",
    "        adapted_model_name                  = f\"{model}-{SPECIES1}\"\n",
    "        ground_truth_model_name             = f\"{model}-{SPECIES2}\"\n",
    "\n",
    "        for tf in TFS:\n",
    "            for the_bound in [adapted_model_name, ground_truth_model_name]:\n",
    "\n",
    "                if \"BM\" not in model and the_bound == ground_truth_model_name:\n",
    "                    continue\n",
    "                else:\n",
    "                    cnf_matrix[the_bound][tf] = {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}\n",
    "\n",
    "                    # (0) First we just calculate the raw confusion matrix for the model\n",
    "                    preds   = preds_dict[the_bound][tf][SPECIES2]\n",
    "                    labels  = labels_dict[the_bound][tf][SPECIES2]\n",
    "\n",
    "                    # We need to categorize the adapted predictions based on the labels\n",
    "                    tp_preds = preds[(preds > 0.5) & (labels == 1)]\n",
    "                    fp_preds = preds[(preds > 0.5) & (labels == 0)]\n",
    "                    tn_preds = preds[(preds <= 0.5) & (labels == 0)]\n",
    "                    fn_preds = preds[(preds <= 0.5) & (labels == 1)]\n",
    "                    \n",
    "                    # (1) Now we add the raw counts for each category to the confusion matrix\n",
    "                    if percents:\n",
    "                        cnf_matrix[the_bound][tf][\"TP\"] = round((tp_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                        cnf_matrix[the_bound][tf][\"FP\"] = round((fp_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                        cnf_matrix[the_bound][tf][\"TN\"] = round((tn_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                        cnf_matrix[the_bound][tf][\"FN\"] = round((fn_preds.shape[0] / len(labels)) * 100, 3)\n",
    "                    else:\n",
    "                        cnf_matrix[the_bound][tf][\"TP\"] = tp_preds.shape[0]\n",
    "                        cnf_matrix[the_bound][tf][\"FP\"] = fp_preds.shape[0]\n",
    "                        cnf_matrix[the_bound][tf][\"TN\"] = tn_preds.shape[0]\n",
    "                        cnf_matrix[the_bound][tf][\"FN\"] = fn_preds.shape[0]\n",
    "\n",
    "                    # (2) We are primarily interested in gauging the differential false positive \n",
    "                    # predictions between the target model to the source model\n",
    "                    if differential:\n",
    "\n",
    "                        # Adapted models overpredict as compared to the ground truth model\n",
    "                        overpred        = set(np.nonzero(preds - preds_dict[ground_truth_model_name][tf][SPECIES2] > 0.5)[0])\n",
    "                        #overpred        = set(np.nonzero(preds - preds_dict[\"BM-mm10\"][tf][SPECIES2] > 0.5)[0])\n",
    "                        overpred_sites  = np.array([False if i not in overpred else True for i, j in enumerate(labels)])\n",
    "\n",
    "                        # Adapted models underpredict as compared to the ground truth model\n",
    "                        underpred       = set(np.nonzero(preds_dict[ground_truth_model_name][tf][SPECIES2] - preds > 0.5)[0])\n",
    "                        #underpred       = set(np.nonzero(preds_dict[\"BM-mm10\"][tf][SPECIES2] - preds > 0.5)[0])\n",
    "                        underpred_list  = np.array([False if i not in underpred else True for i, j in enumerate(labels)])\n",
    "\n",
    "                        if percents:\n",
    "                            cnf_matrix[the_bound][tf]['dTP'] = round((preds[(preds > 0.5) & (labels == 1) & (overpred_sites == True)].shape[0] / tp_preds.shape[0]) * 100, 3)\n",
    "                            cnf_matrix[the_bound][tf]['dFP'] = round((preds[(preds > 0.5) & (labels == 0) & (overpred_sites == True)].shape[0] / fp_preds.shape[0]) * 100, 3)\n",
    "                            cnf_matrix[the_bound][tf]['dTN'] = round((preds[(preds <= 0.5) & (labels == 0) & (underpred_list == True)].shape[0] / tn_preds.shape[0]) * 100, 3)\n",
    "                            cnf_matrix[the_bound][tf]['dFN'] = round((preds[(preds <= 0.5) & (labels == 1) & (underpred_list == True)].shape[0] / fn_preds.shape[0]) * 100, 3)\n",
    "                        else:\n",
    "                            cnf_matrix[the_bound][tf]['dTP'] = preds[(preds > 0.5) & (labels == 1) & (overpred_sites == True)].shape[0] / tp_preds.shape[0]\n",
    "                            cnf_matrix[the_bound][tf]['dFP'] = preds[(preds > 0.5) & (labels == 0) & (overpred_sites == True)].shape[0] / fp_preds.shape[0]\n",
    "                            cnf_matrix[the_bound][tf]['dTN'] = preds[(preds <= 0.5) & (labels == 0) & (underpred_list == True)].shape[0] / tn_preds.shape[0]\n",
    "                            cnf_matrix[the_bound][tf]['dFN'] = preds[(preds <= 0.5) & (labels == 1) & (underpred_list == True)].shape[0] / fn_preds.shape[0]\n",
    "\n",
    "                    # (3) We either get the performance metrics or we don't.\n",
    "                    if performance:\n",
    "                        \n",
    "                        # We cannot just grab the auPRC from the confusion matrix calcs, because\n",
    "                        # we averaged the sigmoid values over the five-folds. We need to load in the \n",
    "                        # performance data and do it manually.\n",
    "                        performance_df = pd.read_csv(f\"{ROOT}/plots/Tables1-2/performance_data.csv\", index_col=None)\n",
    "                        performance_df = performance_df.iloc[:, 1:]\n",
    "                        \n",
    "                        # We need to only grab the columns we need\n",
    "                        performance_model_name = MODEL_NAMES[the_bound]\n",
    "                        auPRC = performance_df.loc[performance_df[\"Model\"] == performance_model_name, :].loc[performance_df[\"Eval\"] == SPECIES2, :].loc[performance_df[\"TF\"] == tf, :].loc[:, \"auPRC\"]\n",
    "                        auPRC = round(np.mean(auPRC), 3)\n",
    "                        \n",
    "                        cnf_matrix[the_bound][tf]['auPRC'] = auPRC\n",
    "\n",
    "    return cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_bed_file(tf, species):\n",
    "    # This function returns the path to a BED-format file\n",
    "    # containing the chromosome names, starts, and ends for\n",
    "    # all examples to test the model with.\n",
    "    # Note this is specific to a TF (binding labels\n",
    "    # are loaded in from this file)!\n",
    "    return(f\"{ROOT}/data/{species}/{tf}/chr2.bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repeat_intersect_file_chr2(species):\n",
    "    # See make_repeat_files.sh for creating this file.\n",
    "    # Basically:\n",
    "    # awk '$1 == \"chr2\"' [repeatmaker alu file] > rmsk_alus_chr2.bed\n",
    "    # bedtools intersect -a [get_test_bed_file(species)] -b rmsk_alus_chr2.bed -u -sorted > chr2_alus_intersect.bed\n",
    "    \n",
    "    # This file should contain all windows in the test data\n",
    "    # that intersect with Alus (this is different from all\n",
    "    # annotated Alus -- model is expecting windows of the\n",
    "    # correct size).\n",
    "\n",
    "\n",
    "    assert species in SPECIES, f\"Species {species} not in {SPECIES}\"\n",
    "    \n",
    "    if species == \"hg38\":\n",
    "        return(f\"{ROOT}/data/{species}/chr2_alus_intersect.bed\")\n",
    "    else:\n",
    "        return(f\"{ROOT}/data/{species}/chr2_b1s_intersect.bed\")\n",
    "\n",
    "def get_window_starts_fast(filename):\n",
    "    # assuming the file is in bed format and col 2 is what we want\n",
    "    df = pd.read_csv(filename, sep='\\t', header=None)\n",
    "    starts = np.array(df[1])\n",
    "    return starts\n",
    "\n",
    "def matches_across_sorted_lists(list_a, list_b):\n",
    "    # this function is NOT symmetric!!!\n",
    "    # the output will have len equal to len of list_a\n",
    "    \n",
    "    # here we assume that list_b is a subset of list_a\n",
    "    # (doesn't contain elements not found in list_a)\n",
    "    matches = []\n",
    "    b_index = 0\n",
    "    for a_item in list_a:\n",
    "        while True:\n",
    "            if b_index >= len(list_b):\n",
    "                matches.append(False)\n",
    "                break\n",
    "            if list_b[b_index] > a_item:\n",
    "                matches.append(False)\n",
    "                break\n",
    "            else:\n",
    "                assert list_b[b_index] == a_item\n",
    "                matches.append(True)\n",
    "                b_index += 1\n",
    "                break\n",
    "    return np.array(matches)\n",
    "   \n",
    "def get_repeat_labels(tf, species):\n",
    "    repeat_starts = get_window_starts_fast(get_repeat_intersect_file_chr2(species=species))\n",
    "\n",
    "    # which tf here doesn't matter; not using labels\n",
    "    all_starts      = get_window_starts_fast(get_test_bed_file(tf=tf, species=species))\n",
    "    repeat_labels   = matches_across_sorted_lists(all_starts, repeat_starts)\n",
    "    repeat_indices  = set(np.nonzero(repeat_labels)[0])\n",
    "    \n",
    "    return repeat_labels, repeat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds_and_labels_dfs(preds_dict, labels_dict, repeat_labels):\n",
    "    preds_dfs = defaultdict(lambda : dict())\n",
    "    \n",
    "    for model in MODELS:\n",
    "        adapted_model_name = f\"{model}-{SPECIES1}\"\n",
    "        for tf in TFS:\n",
    "            dict_to_make_into_df = {\"labels\" : labels_dict[adapted_model_name][tf][SPECIES2]}\n",
    "            goal_len = labels_dict[adapted_model_name][tf][SPECIES2].shape[0]  # assuming labels are already truncated\n",
    "\n",
    "            # Now we get the relevant predictions\n",
    "            model_preds = preds_dict[adapted_model_name][tf][SPECIES2]\n",
    "            assert model_preds.shape[0] == goal_len\n",
    "\n",
    "            dict_to_make_into_df[adapted_model_name] = model_preds\n",
    "\n",
    "            dict_to_make_into_df[\"repeat_labels\"] = repeat_labels[:goal_len]\n",
    "            preds_dfs[tf][adapted_model_name] = pd.DataFrame(dict_to_make_into_df)\n",
    "\n",
    "    return preds_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(cnf_matrix, model1, model2, model3, header=None, row_order=None, caption=None):\n",
    "    print(r'\\begin{table*}[t]{')\n",
    "    print(r'\\centering')\n",
    "\n",
    "    if caption is not None:\n",
    "        print(r'\\caption{' + caption + r'\\label{Tab:01}}')\n",
    "\n",
    "    print(r'\\resizebox{\\textwidth}{!}{')\n",
    "    print(r'\\setlength{\\tabcolsep}{0.8em}')\n",
    "\n",
    "    # We only do differential here\n",
    "    print(r'\\centering \\begin{tabular}{@{}c|cccc@{}}\\toprule')\n",
    "    if header is None:\n",
    "        header = r\"\\textbf{TF} & \\textbf{TPs (\\%)} & \\textbf{FPs (\\%)} & \\textbf{FNs (\\%)} & \\textbf{auPRC}\"\n",
    "        col_order = [\"TP\", \"FP\", \"FN\", \"auPRC\"]\n",
    "        reps=len(col_order)\n",
    "\n",
    "    print(header + r' \\\\')\n",
    "\n",
    "    table_segment = r'\\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source \\end{tabular}'  # Right after 1cm}, |>\n",
    "    line = r'& ' + ' & '.join([table_segment] * (reps)) + r' \\\\'  # Note the parentheses\n",
    "    print(line + r\"\\midrule\")\n",
    "\n",
    "    if row_order is None:\n",
    "        row_order = TFS\n",
    "    last_row = row_order[-1]\n",
    "\n",
    "    # Initialize a dictionary to store sums for each column and model\n",
    "    col_sums = {col: {model1: 0, model2: 0, model3: 0} for col in col_order}\n",
    "\n",
    "    for row_key in row_order:\n",
    "        # Get information from each model\n",
    "        row_model1 = [cnf_matrix[model1][row_key][col] for col in col_order]\n",
    "        row_model2 = [cnf_matrix[model2][row_key][col] for col in col_order]\n",
    "        row_model3 = [cnf_matrix[model3][row_key][col] for col in col_order]\n",
    "\n",
    "        # Accumulate sums for averaging\n",
    "        for i, col in enumerate(col_order):\n",
    "            col_sums[col][model1] += row_model1[i]\n",
    "            col_sums[col][model2] += row_model2[i]\n",
    "            col_sums[col][model3] += row_model3[i]\n",
    "\n",
    "        # Find the best performing model for each column\n",
    "        best_models = {}\n",
    "        for col_idx, col in enumerate(col_order):\n",
    "            values = [row_model1[col_idx], row_model2[col_idx], row_model3[col_idx]]\n",
    "            if col == \"FP\" or col == \"FN\":\n",
    "                best_idx = np.argmin(values)\n",
    "            else:\n",
    "                best_idx = np.argmax(values)\n",
    "            \n",
    "            if best_idx == 0:\n",
    "                best_models[col] = model1\n",
    "            elif best_idx == 1:\n",
    "                best_models[col] = model2\n",
    "            else:\n",
    "                best_models[col] = model3\n",
    "\n",
    "        # Format them as strings, bolding the best\n",
    "        row_model1_as_str = [\n",
    "            (r\"\\textbf{\" + str(round(num, 3)) + r\"}\" if model1 == best_models[col] else str(round(num, 3)))\n",
    "            for num, col in zip(row_model1, col_order)\n",
    "        ]\n",
    "        row_model2_as_str = [\n",
    "            (r\"\\textbf{\" + str(round(num, 3)) + r\"}\" if model2 == best_models[col] else str(round(num, 3)))\n",
    "            for num, col in zip(row_model2, col_order)\n",
    "        ]\n",
    "        row_model3_as_str = [\n",
    "            (r\"\\textbf{\" + str(round(num, 3)) + r\"}\" if model3 == best_models[col] else str(round(num, 3)))\n",
    "            for num, col in zip(row_model3, col_order)\n",
    "        ]\n",
    "\n",
    "        # Combine their information together\n",
    "        combined_row = [\n",
    "            {model1: i[0], model2: i[1], model3: i[2]}\n",
    "            for i in zip(row_model1_as_str, row_model2_as_str, row_model3_as_str)\n",
    "        ]\n",
    "        combine_row_as_str = [\n",
    "            r\"\\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}\"\n",
    "            + f\"{i[model1]} & {i[model2]} & {i[model3]}\"\n",
    "            r\"\\end{tabular}\"\n",
    "            for i in combined_row\n",
    "        ]\n",
    "        tf_fancy_name = TFS[TFS.index(row_key)]\n",
    "\n",
    "        print(tf_fancy_name + \" & \" + \" & \".join(combine_row_as_str) + r' \\\\')\n",
    "\n",
    "    # Calculate and print the average row\n",
    "    avg_row = {}\n",
    "    num_rows = len(row_order)\n",
    "    for col in col_order:\n",
    "        avg_row[col] = {\n",
    "            model: round(col_sums[col][model] / num_rows, 3) for model in [model1, model2, model3]\n",
    "        }\n",
    "\n",
    "    # Find the best performing model for each column in the average row\n",
    "    best_avg_models = {}\n",
    "    for col in col_order:\n",
    "        values = [avg_row[col][model1], avg_row[col][model2], avg_row[col][model3]]\n",
    "        if col == \"FP\" or col == \"FN\":\n",
    "            best_idx = np.argmin(values)\n",
    "        else:\n",
    "            best_idx = np.argmax(values)\n",
    "            \n",
    "        if best_idx == 0:\n",
    "            best_avg_models[col] = model1\n",
    "        elif best_idx == 1:\n",
    "            best_avg_models[col] = model2\n",
    "        else:\n",
    "            best_avg_models[col] = model3\n",
    "\n",
    "    # Format the average row as strings, bolding the best\n",
    "    avg_row_as_str = []\n",
    "    for col in col_order:\n",
    "        avg_row_as_str.append(\n",
    "            r\"\\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}\"\n",
    "            + (\n",
    "                r\"\\textbf{\"\n",
    "                + str(avg_row[col][model1])\n",
    "                + r\"}\"\n",
    "                if model1 == best_avg_models[col]\n",
    "                else str(avg_row[col][model1])\n",
    "            )\n",
    "            + \" & \"\n",
    "            + (\n",
    "                r\"\\textbf{\"\n",
    "                + str(avg_row[col][model2])\n",
    "                + r\"}\"\n",
    "                if model2 == best_avg_models[col]\n",
    "                else str(avg_row[col][model2])\n",
    "            )\n",
    "            + \" & \"\n",
    "            + (\n",
    "                r\"\\textbf{\"\n",
    "                + str(avg_row[col][model3])\n",
    "                + r\"}\"\n",
    "                if model3 == best_avg_models[col]\n",
    "                else str(avg_row[col][model3])\n",
    "            )\n",
    "            + r\"\\end{tabular}\"\n",
    "        )\n",
    "\n",
    "    print(r\"\\midrule\")\n",
    "    print(r\"Average & \" + \" & \".join(avg_row_as_str) + r' \\\\\\bottomrule')\n",
    "\n",
    "    print(r'\\end{tabular}}{}')\n",
    "    print(r'\\end{table*}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dict, labels_dict, bound_indices, unbound_indices = load_fivefold_data(average=True, verbose=False)\n",
    "repeat_labels, repeat_indices                           = get_repeat_labels(tf=TFS[0], species=SPECIES2)\n",
    "preds_dfs                                               = make_preds_and_labels_dfs(preds_dict, labels_dict, repeat_labels)\n",
    "cnf_matrix                                              = generate_confusion_matrix(preds_dict, labels_dict, percents=True, differential=False, performance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[t]{\n",
      "\\centering\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\setlength{\\tabcolsep}{0.8em}\n",
      "\\centering \\begin{tabular}{@{}c|cccc@{}}\\toprule\n",
      "\\textbf{TF} & \\textbf{TPs (\\%)} & \\textbf{FPs (\\%)} & \\textbf{FNs (\\%)} & \\textbf{auPRC} \\\\\n",
      "& \\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source \\end{tabular} & \\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source \\end{tabular} & \\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source \\end{tabular} & \\begin{tabular}[c]{>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}>{\\centering\\arraybackslash}p{1cm}}GRL & MORALE & Source \\end{tabular} \\\\\\midrule\n",
      "CTCF & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}\\textbf{0.7} & 0.699 & 0.697\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}3.937 & \\textbf{3.237} & 3.591\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}\\textbf{0.042} & 0.044 & 0.046\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.56 & \\textbf{0.62} & 0.528\\end{tabular} \\\\\n",
      "CEBPA & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.988 & \\textbf{0.996} & 0.952\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}7.158 & 6.66 & \\textbf{5.732}\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.335 & \\textbf{0.328} & 0.372\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.251 & \\textbf{0.271} & 0.27\\end{tabular} \\\\\n",
      "HNF4A & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.824 & \\textbf{0.828} & 0.813\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}11.026 & 10.388 & \\textbf{9.802}\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.122 & \\textbf{0.118} & 0.133\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.217 & \\textbf{0.233} & 0.23\\end{tabular} \\\\\n",
      "RXRA & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.831 & 0.827 & \\textbf{0.843}\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}17.891 & \\textbf{16.039} & 19.208\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.087 & 0.091 & \\textbf{0.075}\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.19 & 0.205 & \\textbf{0.21}\\end{tabular} \\\\\n",
      "\\midrule\n",
      "Average & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.836 & \\textbf{0.837} & 0.826\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}10.003 & \\textbf{9.081} & 9.583\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.146 & \\textbf{0.145} & 0.156\\end{tabular} & \\begin{tabular}[c]{>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}>{\\raggedleft\\arraybackslash}p{1cm}}0.304 & \\textbf{0.332} & 0.31\\end{tabular} \\\\\\bottomrule\n",
      "\\end{tabular}}{}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "print_table(\n",
    "    cnf_matrix=cnf_matrix,\n",
    "    model1=f\"GRL-{SPECIES1}\",\n",
    "    model2=f\"MORALE-{SPECIES1}\",\n",
    "    model3=f\"BM-{SPECIES1}\",\n",
    "    header=None,\n",
    "    row_order=None,\n",
    "    caption=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
